
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pietrolesci.github.io/energizer/api/acquisition_functions/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.4.3">
    
    
      
        <title>Acquisition function - Energizer</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6b80c2a2.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#energizer.acquisition_functions" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
              <button class="md-banner__button md-icon" aria-label="Don't show this again">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
              </button>
            
            
<span> For updates follow
  <a href="https://twitter.com/pietro_lesci">
    <strong style="color: white;">@pietro_lesci</strong>
  </a>
  on
  <span class="twemoji twitter">
    <a href="https://twitter.com/pietro_lesci" style="color: #00acee">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
        <!-- Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.-->
        <path
          d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z">
        </path>
      </svg>
    </a>
  </span>
  <strong>Twitter</strong>
</span>

          </div>
          
            <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script>
          
        </aside>
      
    </div>
    
      <div data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Energizer" class="md-header__button md-logo" aria-label="Energizer" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Energizer
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Acquisition function
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_3" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="teal"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="teal"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_3">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/pietrolesci/energizer" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    pietrolesci/energizer
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../installation/" class="md-tabs__link">
      Installation
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../trainer/" class="md-tabs__link md-tabs__link--active">
        API
      </a>
    </li>
  

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../contributing/" class="md-tabs__link">
      Contributing
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../changelog/" class="md-tabs__link">
      Changelog
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../about/" class="md-tabs__link">
      About
    </a>
  </li>

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Energizer" class="md-nav__button md-logo" aria-label="Energizer" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Energizer
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pietrolesci/energizer" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    pietrolesci/energizer
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="API" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../trainer/" class="md-nav__link">
        Trainer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../query_strategies/" class="md-nav__link">
        Query strategies
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Acquisition function
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Acquisition function
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions" class="md-nav__link">
    energizer.acquisition_functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.bald" class="md-nav__link">
    bald()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.confidence" class="md-nav__link">
    confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.entropy" class="md-nav__link">
    entropy()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.expected_confidence" class="md-nav__link">
    expected_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.expected_entropy" class="md-nav__link">
    expected_entropy()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.expected_least_confidence" class="md-nav__link">
    expected_least_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.expected_margin_confidence" class="md-nav__link">
    expected_margin_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.least_confidence" class="md-nav__link">
    least_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.margin_confidence" class="md-nav__link">
    margin_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.predictive_entropy" class="md-nav__link">
    predictive_entropy()
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../contributing/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../changelog/" class="md-nav__link">
        Changelog
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions" class="md-nav__link">
    energizer.acquisition_functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.bald" class="md-nav__link">
    bald()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.confidence" class="md-nav__link">
    confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.entropy" class="md-nav__link">
    entropy()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.expected_confidence" class="md-nav__link">
    expected_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.expected_entropy" class="md-nav__link">
    expected_entropy()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.expected_least_confidence" class="md-nav__link">
    expected_least_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.expected_margin_confidence" class="md-nav__link">
    expected_margin_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.least_confidence" class="md-nav__link">
    least_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.margin_confidence" class="md-nav__link">
    margin_confidence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#energizer.acquisition_functions.predictive_entropy" class="md-nav__link">
    predictive_entropy()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
  



  <a href="https://github.com/pietrolesci/energizer/tree/main/docs/api/acquisition_functions.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


  <h1>Acquisition function</h1>

<div class="doc doc-object doc-module">


<a id="energizer.acquisition_functions"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.bald" class="doc doc-heading">
<code class="highlight language-python"><span class="n">bald</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.bald" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Compute the BALD acquisition function.</p>
<p>NOTE: this could have been simply implemented as</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="n">predictive_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">-</span> <span class="n">expected_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>however, both functions would need to compute the softmax internally.
To avoid doubling the computation uselessly, we implement this function
so that it only computes the softmax ones.</p>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-0-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-0-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-0-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-0-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-0-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-93" name="__codelineno-0-93"></a><span class="k">def</span> <span class="nf">bald</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the BALD acquisition function.</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a><span class="sd">    NOTE: this could have been simply implemented as</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a><span class="sd">    ```python</span>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a><span class="sd">    predictive_entropy(logits) - expected_entropy(logits)</span>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a><span class="sd">    ```</span>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a><span class="sd">    however, both functions would need to compute the softmax internally.</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a><span class="sd">    To avoid doubling the computation uselessly, we implement this function</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a><span class="sd">    so that it only computes the softmax ones.</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a>    <span class="c1"># predictive_entropy(logits) - expected_entropy(logits)</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a>    <span class="n">probs</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>    <span class="c1"># To get the first term, we make many runs, average the output, and measure the entropy.</span>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a>    <span class="n">predictive_entropy</span> <span class="o">=</span> <span class="n">entr</span><span class="p">(</span><span class="n">probs</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a>    <span class="c1"># To get the second term, we make many runs, measure the entropy of every run, and take the average.</span>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a>    <span class="n">expected_entropy</span> <span class="o">=</span> <span class="n">entr</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a>    <span class="k">return</span> <span class="n">predictive_entropy</span> <span class="o">-</span> <span class="n">expected_entropy</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.confidence" class="doc doc-heading">
<code class="highlight language-python"><span class="n">confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.confidence" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Computes confidence based on logits.</p>
<p>Computes the confidence defined as the highest probability the model assigns to a class, that is</p>
<div class="arithmatex">\[\max_c p_{bc}\]</div>
<p>where <span class="arithmatex">\(p_{bc}\)</span> is the probability for class <span class="arithmatex">\(c\)</span> for instance <span class="arithmatex">\(b\)</span> in the batch.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>logits</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>A tensor of dimensions <code>(B: batch_size, C: num_classes)</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>k</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The "k" in "top-k".</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The confidence defined as the maximum probability assigned to a class, i.e. a vector of</p></td>
        </tr>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>dimensions <code>(B: batch_size, k)</code>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-119" name="__codelineno-0-119"></a><span class="k">def</span> <span class="nf">confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes confidence based on logits.</span>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a><span class="sd">    Computes the confidence defined as the highest probability the model assigns to a class, that is</span>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a><span class="sd">    $$\max_c p_{bc}$$</span>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a><span class="sd">    where $p_{bc}$ is the probability for class $c$ for instance $b$ in the batch.</span>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a><span class="sd">        logits (Tensor): A tensor of dimensions `(B: batch_size, C: num_classes)`.</span>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a><span class="sd">        k (int): The &quot;k&quot; in &quot;top-k&quot;.</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a><span class="sd">        The confidence defined as the maximum probability assigned to a class, i.e. a vector of</span>
<a id="__codelineno-0-134" name="__codelineno-0-134"></a><span class="sd">        dimensions `(B: batch_size, k)`.</span>
<a id="__codelineno-0-135" name="__codelineno-0-135"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a>    <span class="n">probs</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.entropy" class="doc doc-heading">
<code class="highlight language-python"><span class="n">entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.entropy" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Computes Shannon's entropy in nats.</p>
<p>It expects a tensor of logits with the following dimensions: <code>(B: batch_size, C: num_classes)</code>.</p>
<p>This function implements the following steps, for each element along the <code>B: batch_size</code> dimension:</p>
<ul>
<li>
<p>Converts logits in probabilities along the <code>C: num_classes</code> dimension
<span class="arithmatex">\(<span class="arithmatex">\(p_{bc} = e^{l_{bc}} / \sum_j e^{l_{bj}}\)</span>\)</span></p>
</li>
<li>
<p>Computes Shannon's entropy along the <code>C: num_classes</code> dimension
<span class="arithmatex">\(<span class="arithmatex">\(\mathrm{H}_b\left(\mathrm{p}(X)\right) = - \sum_c p_{bc} \log(p_{bc})\)</span>\)</span></p>
</li>
</ul>
<p>where <span class="arithmatex">\(l_{bc}\)</span> is the logit for class <span class="arithmatex">\(c\)</span> for the <span class="arithmatex">\(b\)</span>-th element in the batch, and <span class="arithmatex">\(\mathrm{p}\)</span> is a
probability mass function for a random variable <span class="arithmatex">\(X\)</span> such that <span class="arithmatex">\(\mathrm{p}(X = c) = p_c\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>logits</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>A tensor of dimensions <code>(B: batch_size, C: num_classes)</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The Shannon's entropy, i.e. a vector of dimensions <code>(B: batch_size,)</code>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-0-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-0-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-0-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-0-10">10</a></span>
<span class="normal"><a href="#__codelineno-0-11">11</a></span>
<span class="normal"><a href="#__codelineno-0-12">12</a></span>
<span class="normal"><a href="#__codelineno-0-13">13</a></span>
<span class="normal"><a href="#__codelineno-0-14">14</a></span>
<span class="normal"><a href="#__codelineno-0-15">15</a></span>
<span class="normal"><a href="#__codelineno-0-16">16</a></span>
<span class="normal"><a href="#__codelineno-0-17">17</a></span>
<span class="normal"><a href="#__codelineno-0-18">18</a></span>
<span class="normal"><a href="#__codelineno-0-19">19</a></span>
<span class="normal"><a href="#__codelineno-0-20">20</a></span>
<span class="normal"><a href="#__codelineno-0-21">21</a></span>
<span class="normal"><a href="#__codelineno-0-22">22</a></span>
<span class="normal"><a href="#__codelineno-0-23">23</a></span>
<span class="normal"><a href="#__codelineno-0-24">24</a></span>
<span class="normal"><a href="#__codelineno-0-25">25</a></span>
<span class="normal"><a href="#__codelineno-0-26">26</a></span>
<span class="normal"><a href="#__codelineno-0-27">27</a></span>
<span class="normal"><a href="#__codelineno-0-28">28</a></span>
<span class="normal"><a href="#__codelineno-0-29">29</a></span>
<span class="normal"><a href="#__codelineno-0-30">30</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-6" name="__codelineno-0-6"></a><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-7" name="__codelineno-0-7"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes Shannon&#39;s entropy in nats.</span>
<a id="__codelineno-0-8" name="__codelineno-0-8"></a>
<a id="__codelineno-0-9" name="__codelineno-0-9"></a><span class="sd">    It expects a tensor of logits with the following dimensions: `(B: batch_size, C: num_classes)`.</span>
<a id="__codelineno-0-10" name="__codelineno-0-10"></a>
<a id="__codelineno-0-11" name="__codelineno-0-11"></a><span class="sd">    This function implements the following steps, for each element along the `B: batch_size` dimension:</span>
<a id="__codelineno-0-12" name="__codelineno-0-12"></a>
<a id="__codelineno-0-13" name="__codelineno-0-13"></a><span class="sd">    - Converts logits in probabilities along the `C: num_classes` dimension</span>
<a id="__codelineno-0-14" name="__codelineno-0-14"></a><span class="sd">    $$p_{bc} = e^{l_{bc}} / \sum_j e^{l_{bj}}$$</span>
<a id="__codelineno-0-15" name="__codelineno-0-15"></a>
<a id="__codelineno-0-16" name="__codelineno-0-16"></a>
<a id="__codelineno-0-17" name="__codelineno-0-17"></a><span class="sd">    - Computes Shannon&#39;s entropy along the `C: num_classes` dimension</span>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a><span class="sd">    $$\mathrm{H}_b\left(\mathrm{p}(X)\right) = - \sum_c p_{bc} \log(p_{bc})$$</span>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a><span class="sd">    where $l_{bc}$ is the logit for class $c$ for the $b$-th element in the batch, and $\mathrm{p}$ is a</span>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a><span class="sd">    probability mass function for a random variable $X$ such that $\mathrm{p}(X = c) = p_c$.</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a><span class="sd">        logits (Tensor): A tensor of dimensions `(B: batch_size, C: num_classes)`.</span>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a><span class="sd">        The Shannon&#39;s entropy, i.e. a vector of dimensions `(B: batch_size,)`.</span>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a>    <span class="n">probs</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a>    <span class="k">return</span> <span class="n">entr</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># remember: you need to sum across classes</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.expected_confidence" class="doc doc-heading">
<code class="highlight language-python"><span class="n">expected_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.expected_confidence" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Computes the expected confidence based on logits.</p>
<p>Computes the expected confidence across samples, defined as the highest probability the model assigns
to a class, that is</p>
<div class="arithmatex">\[\sum_s \max_c p_{bcs}\]</div>
<p>where <span class="arithmatex">\(p_{bcs}\)</span> is the probability for class <span class="arithmatex">\(c\)</span> for instance <span class="arithmatex">\(b\)</span> in batch of sample <span class="arithmatex">\(s\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>logits</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>A tensor of dimensions <code>(B: batch_size, C: num_classes, S: num_samples)</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>k</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The "k" in "top-k".</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The confidence defined as the maximum probability assigned to a class, i.e. a vector of</p></td>
        </tr>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>dimensions <code>(B: batch_size, k)</code>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="k">def</span> <span class="nf">expected_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the expected confidence based on logits.</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>
<a id="__codelineno-0-143" name="__codelineno-0-143"></a><span class="sd">    Computes the expected confidence across samples, defined as the highest probability the model assigns</span>
<a id="__codelineno-0-144" name="__codelineno-0-144"></a><span class="sd">    to a class, that is</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a><span class="sd">    $$\sum_s \max_c p_{bcs}$$</span>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">    where $p_{bcs}$ is the probability for class $c$ for instance $b$ in batch of sample $s$.</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a><span class="sd">        logits (Tensor): A tensor of dimensions `(B: batch_size, C: num_classes, S: num_samples)`.</span>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a><span class="sd">        k (int): The &quot;k&quot; in &quot;top-k&quot;.</span>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-155" name="__codelineno-0-155"></a><span class="sd">        The confidence defined as the maximum probability assigned to a class, i.e. a vector of</span>
<a id="__codelineno-0-156" name="__codelineno-0-156"></a><span class="sd">        dimensions `(B: batch_size, k)`.</span>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a>    <span class="n">probs</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-159" name="__codelineno-0-159"></a>    <span class="n">confidence</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<a id="__codelineno-0-160" name="__codelineno-0-160"></a>    <span class="k">return</span> <span class="n">confidence</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.expected_entropy" class="doc doc-heading">
<code class="highlight language-python"><span class="n">expected_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.expected_entropy" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Computes the expected Shannon's entropy in nats.</p>
<p>It expects a tensor of logits with the following dimensions: <code>(B: batch_size, C: num_classes, S: num_inference_iterations)</code>.
This function implements the following steps, for each element along the <code>B: batch_size</code> dimension:</p>
<ul>
<li>
<p>Converts logits in probabilities along the <code>C: num_classes</code> dimension
<span class="arithmatex">\(<span class="arithmatex">\(p_{bcs} = e^{l_{bcs}} / \sum_j e^{l_{bjs}}\)</span>\)</span></p>
</li>
<li>
<p>Computes Shannon's entropy along the <code>C: num_classes</code> dimension
<span class="arithmatex">\(<span class="arithmatex">\(\mathrm{H}_{bs}\left(\mathrm{p}(X) \right) = - \sum_c p_{bcs} \log(p_{bcs})\)</span>\)</span></p>
</li>
<li>
<p>Averages the Shannon's entropy along the <code>S: num_samples</code> dimension
<span class="arithmatex">\(<span class="arithmatex">\(\frac{1}{S} \sum_s \mathrm{H}_{bs}\left(\mathrm{p}(X)\right)\)</span>\)</span></p>
</li>
</ul>
<p>where <span class="arithmatex">\(l_{bcs}\)</span> is the logit for class <span class="arithmatex">\(c\)</span> for the <span class="arithmatex">\(b\)</span>-th element in the batch in the <span class="arithmatex">\(s\)</span>-th sample,
and <span class="arithmatex">\(\mathrm{p}\)</span> is a probability mass function for a random variable <span class="arithmatex">\(X\)</span> such that <span class="arithmatex">\(\mathrm{p}(X = c) = p_c\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>logits</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>A tensor of dimensions <code>(B: batch_size, C: num_classes, S: num_inference_iterations)</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The Shannon's entropy, i.e. a vector of dimensions <code>(B: batch_size,)</code>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-64">64</a></span>
<span class="normal"><a href="#__codelineno-0-65">65</a></span>
<span class="normal"><a href="#__codelineno-0-66">66</a></span>
<span class="normal"><a href="#__codelineno-0-67">67</a></span>
<span class="normal"><a href="#__codelineno-0-68">68</a></span>
<span class="normal"><a href="#__codelineno-0-69">69</a></span>
<span class="normal"><a href="#__codelineno-0-70">70</a></span>
<span class="normal"><a href="#__codelineno-0-71">71</a></span>
<span class="normal"><a href="#__codelineno-0-72">72</a></span>
<span class="normal"><a href="#__codelineno-0-73">73</a></span>
<span class="normal"><a href="#__codelineno-0-74">74</a></span>
<span class="normal"><a href="#__codelineno-0-75">75</a></span>
<span class="normal"><a href="#__codelineno-0-76">76</a></span>
<span class="normal"><a href="#__codelineno-0-77">77</a></span>
<span class="normal"><a href="#__codelineno-0-78">78</a></span>
<span class="normal"><a href="#__codelineno-0-79">79</a></span>
<span class="normal"><a href="#__codelineno-0-80">80</a></span>
<span class="normal"><a href="#__codelineno-0-81">81</a></span>
<span class="normal"><a href="#__codelineno-0-82">82</a></span>
<span class="normal"><a href="#__codelineno-0-83">83</a></span>
<span class="normal"><a href="#__codelineno-0-84">84</a></span>
<span class="normal"><a href="#__codelineno-0-85">85</a></span>
<span class="normal"><a href="#__codelineno-0-86">86</a></span>
<span class="normal"><a href="#__codelineno-0-87">87</a></span>
<span class="normal"><a href="#__codelineno-0-88">88</a></span>
<span class="normal"><a href="#__codelineno-0-89">89</a></span>
<span class="normal"><a href="#__codelineno-0-90">90</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-64" name="__codelineno-0-64"></a><span class="k">def</span> <span class="nf">expected_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the expected Shannon&#39;s entropy in nats.</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a><span class="sd">    It expects a tensor of logits with the following dimensions: `(B: batch_size, C: num_classes, S: num_inference_iterations)`.</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a><span class="sd">    This function implements the following steps, for each element along the `B: batch_size` dimension:</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a><span class="sd">    - Converts logits in probabilities along the `C: num_classes` dimension</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a><span class="sd">    $$p_{bcs} = e^{l_{bcs}} / \sum_j e^{l_{bjs}}$$</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a><span class="sd">    - Computes Shannon&#39;s entropy along the `C: num_classes` dimension</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a><span class="sd">    $$\mathrm{H}_{bs}\left(\mathrm{p}(X) \right) = - \sum_c p_{bcs} \log(p_{bcs})$$</span>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a><span class="sd">    - Averages the Shannon&#39;s entropy along the `S: num_samples` dimension</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a><span class="sd">    $$\frac{1}{S} \sum_s \mathrm{H}_{bs}\left(\mathrm{p}(X)\right)$$</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a><span class="sd">    where $l_{bcs}$ is the logit for class $c$ for the $b$-th element in the batch in the $s$-th sample,</span>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a><span class="sd">    and $\mathrm{p}$ is a probability mass function for a random variable $X$ such that $\mathrm{p}(X = c) = p_c$.</span>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a><span class="sd">        logits (Tensor): A tensor of dimensions `(B: batch_size, C: num_classes, S: num_inference_iterations)`.</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a><span class="sd">        The Shannon&#39;s entropy, i.e. a vector of dimensions `(B: batch_size,)`.</span>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a>    <span class="n">probs</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>    <span class="n">entropies</span> <span class="o">=</span> <span class="n">entr</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a>    <span class="k">return</span> <span class="n">entropies</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.expected_least_confidence" class="doc doc-heading">
<code class="highlight language-python"><span class="n">expected_least_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.expected_least_confidence" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Implements the least confidence acquisition function.</p>
<p>References: http://burrsettles.com/pub/settles.activelearning.pdf.</p>
<p>This strategy allows an active learner to select the unlabeled data
samples for which the model is least confident (i.e., most uncertain)
in prediction or class assignment.</p>
<p>It selects an instance <span class="arithmatex">\(x\)</span> such that</p>
<div class="arithmatex">\[\arg \max_{x} \; 1 - p(y_{max}|x, \theta)\]</div>
<p>where <span class="arithmatex">\(y_{max} = \arg\max_y p(y|x, \theta)\)</span>, i.e. the class label with the
highest posterior probability under the model <span class="arithmatex">\(\theta\)</span>. One way to interpret
this uncertainty measure is the expected 0/1-loss, i.e., the model's belief
that it will mislabel <span class="arithmatex">\(x\)</span>.</p>
<p>If samples from a posterior distributions are provided, it computes</p>
<div class="arithmatex">\[\arg \max_{x} \; 1 - \mathrm{E}_{p(\theta| D)} p(y_{max}|x, \theta)\]</div>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-188">188</a></span>
<span class="normal"><a href="#__codelineno-0-189">189</a></span>
<span class="normal"><a href="#__codelineno-0-190">190</a></span>
<span class="normal"><a href="#__codelineno-0-191">191</a></span>
<span class="normal"><a href="#__codelineno-0-192">192</a></span>
<span class="normal"><a href="#__codelineno-0-193">193</a></span>
<span class="normal"><a href="#__codelineno-0-194">194</a></span>
<span class="normal"><a href="#__codelineno-0-195">195</a></span>
<span class="normal"><a href="#__codelineno-0-196">196</a></span>
<span class="normal"><a href="#__codelineno-0-197">197</a></span>
<span class="normal"><a href="#__codelineno-0-198">198</a></span>
<span class="normal"><a href="#__codelineno-0-199">199</a></span>
<span class="normal"><a href="#__codelineno-0-200">200</a></span>
<span class="normal"><a href="#__codelineno-0-201">201</a></span>
<span class="normal"><a href="#__codelineno-0-202">202</a></span>
<span class="normal"><a href="#__codelineno-0-203">203</a></span>
<span class="normal"><a href="#__codelineno-0-204">204</a></span>
<span class="normal"><a href="#__codelineno-0-205">205</a></span>
<span class="normal"><a href="#__codelineno-0-206">206</a></span>
<span class="normal"><a href="#__codelineno-0-207">207</a></span>
<span class="normal"><a href="#__codelineno-0-208">208</a></span>
<span class="normal"><a href="#__codelineno-0-209">209</a></span>
<span class="normal"><a href="#__codelineno-0-210">210</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-188" name="__codelineno-0-188"></a><span class="k">def</span> <span class="nf">expected_least_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-189" name="__codelineno-0-189"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements the least confidence acquisition function.</span>
<a id="__codelineno-0-190" name="__codelineno-0-190"></a>
<a id="__codelineno-0-191" name="__codelineno-0-191"></a><span class="sd">    References: http://burrsettles.com/pub/settles.activelearning.pdf.</span>
<a id="__codelineno-0-192" name="__codelineno-0-192"></a>
<a id="__codelineno-0-193" name="__codelineno-0-193"></a><span class="sd">    This strategy allows an active learner to select the unlabeled data</span>
<a id="__codelineno-0-194" name="__codelineno-0-194"></a><span class="sd">    samples for which the model is least confident (i.e., most uncertain)</span>
<a id="__codelineno-0-195" name="__codelineno-0-195"></a><span class="sd">    in prediction or class assignment.</span>
<a id="__codelineno-0-196" name="__codelineno-0-196"></a>
<a id="__codelineno-0-197" name="__codelineno-0-197"></a><span class="sd">    It selects an instance $x$ such that</span>
<a id="__codelineno-0-198" name="__codelineno-0-198"></a>
<a id="__codelineno-0-199" name="__codelineno-0-199"></a><span class="sd">    $$\arg \max_{x} \; 1 - p(y_{max}|x, \theta)$$</span>
<a id="__codelineno-0-200" name="__codelineno-0-200"></a>
<a id="__codelineno-0-201" name="__codelineno-0-201"></a><span class="sd">    where $y_{max} = \arg\max_y p(y|x, \theta)$, i.e. the class label with the</span>
<a id="__codelineno-0-202" name="__codelineno-0-202"></a><span class="sd">    highest posterior probability under the model $\theta$. One way to interpret</span>
<a id="__codelineno-0-203" name="__codelineno-0-203"></a><span class="sd">    this uncertainty measure is the expected 0/1-loss, i.e., the model&#39;s belief</span>
<a id="__codelineno-0-204" name="__codelineno-0-204"></a><span class="sd">    that it will mislabel $x$.</span>
<a id="__codelineno-0-205" name="__codelineno-0-205"></a>
<a id="__codelineno-0-206" name="__codelineno-0-206"></a><span class="sd">    If samples from a posterior distributions are provided, it computes</span>
<a id="__codelineno-0-207" name="__codelineno-0-207"></a>
<a id="__codelineno-0-208" name="__codelineno-0-208"></a><span class="sd">    $$\arg \max_{x} \; 1 - \mathrm{E}_{p(\theta| D)} p(y_{max}|x, \theta)$$</span>
<a id="__codelineno-0-209" name="__codelineno-0-209"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-210" name="__codelineno-0-210"></a>    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">expected_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.expected_margin_confidence" class="doc doc-heading">
<code class="highlight language-python"><span class="n">expected_margin_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.expected_margin_confidence" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Implements the margin strategy.</p>
<p>Reference: http://burrsettles.com/pub/settles.activelearning.pdf.</p>
<p>Margin sampling aims to correct for a shortcoming in least
confident strategy, by incorporating the posterior of the second most likely label.
Intuitively, instances with large margins are easy, since the classifier has little
doubt in differentiating between the two most likely class labels. Instances with
small margins are more ambiguous, thus knowing the true label would help the
model discriminate more effectively between them.</p>
<p>It selects an instance <span class="arithmatex">\(x\)</span> such that</p>
<div class="arithmatex">\[\arg\min_{x} P(y_1|x, \theta) - P(y_2|x, \theta)\]</div>
<p>where <span class="arithmatex">\(y_1\)</span> and <span class="arithmatex">\(y_2\)</span> are the first and second most probable class labels under the
model defined by <span class="arithmatex">\(\theta\)</span>, respectively.</p>
<p>If samples from a posterior distributions are provided, it computes</p>
<div class="arithmatex">\[\arg\min_{x} \mathrm{E}_{p(\theta| D)} P(y_1|x, \theta) - \mathrm{E}_{p(\theta| D)} P(y_2|x, \theta)\]</div>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-241">241</a></span>
<span class="normal"><a href="#__codelineno-0-242">242</a></span>
<span class="normal"><a href="#__codelineno-0-243">243</a></span>
<span class="normal"><a href="#__codelineno-0-244">244</a></span>
<span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span>
<span class="normal"><a href="#__codelineno-0-265">265</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-241" name="__codelineno-0-241"></a><span class="k">def</span> <span class="nf">expected_margin_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
<a id="__codelineno-0-242" name="__codelineno-0-242"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements the margin strategy.</span>
<a id="__codelineno-0-243" name="__codelineno-0-243"></a>
<a id="__codelineno-0-244" name="__codelineno-0-244"></a><span class="sd">    Reference: http://burrsettles.com/pub/settles.activelearning.pdf.</span>
<a id="__codelineno-0-245" name="__codelineno-0-245"></a>
<a id="__codelineno-0-246" name="__codelineno-0-246"></a><span class="sd">    Margin sampling aims to correct for a shortcoming in least</span>
<a id="__codelineno-0-247" name="__codelineno-0-247"></a><span class="sd">    confident strategy, by incorporating the posterior of the second most likely label.</span>
<a id="__codelineno-0-248" name="__codelineno-0-248"></a><span class="sd">    Intuitively, instances with large margins are easy, since the classifier has little</span>
<a id="__codelineno-0-249" name="__codelineno-0-249"></a><span class="sd">    doubt in differentiating between the two most likely class labels. Instances with</span>
<a id="__codelineno-0-250" name="__codelineno-0-250"></a><span class="sd">    small margins are more ambiguous, thus knowing the true label would help the</span>
<a id="__codelineno-0-251" name="__codelineno-0-251"></a><span class="sd">    model discriminate more effectively between them.</span>
<a id="__codelineno-0-252" name="__codelineno-0-252"></a>
<a id="__codelineno-0-253" name="__codelineno-0-253"></a><span class="sd">    It selects an instance $x$ such that</span>
<a id="__codelineno-0-254" name="__codelineno-0-254"></a>
<a id="__codelineno-0-255" name="__codelineno-0-255"></a><span class="sd">    $$\arg\min_{x} P(y_1|x, \theta) - P(y_2|x, \theta)$$</span>
<a id="__codelineno-0-256" name="__codelineno-0-256"></a>
<a id="__codelineno-0-257" name="__codelineno-0-257"></a><span class="sd">    where $y_1$ and $y_2$ are the first and second most probable class labels under the</span>
<a id="__codelineno-0-258" name="__codelineno-0-258"></a><span class="sd">    model defined by $\theta$, respectively.</span>
<a id="__codelineno-0-259" name="__codelineno-0-259"></a>
<a id="__codelineno-0-260" name="__codelineno-0-260"></a><span class="sd">    If samples from a posterior distributions are provided, it computes</span>
<a id="__codelineno-0-261" name="__codelineno-0-261"></a>
<a id="__codelineno-0-262" name="__codelineno-0-262"></a><span class="sd">    $$\arg\min_{x} \mathrm{E}_{p(\theta| D)} P(y_1|x, \theta) - \mathrm{E}_{p(\theta| D)} P(y_2|x, \theta)$$</span>
<a id="__codelineno-0-263" name="__codelineno-0-263"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-264" name="__codelineno-0-264"></a>    <span class="n">confidence_top2</span> <span class="o">=</span> <span class="n">expected_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-265" name="__codelineno-0-265"></a>    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">confidence_top2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">confidence_top2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.least_confidence" class="doc doc-heading">
<code class="highlight language-python"><span class="n">least_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.least_confidence" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Implements the least confidence acquisition function.</p>
<p>References: http://burrsettles.com/pub/settles.activelearning.pdf.</p>
<p>This strategy allows an active learner to select the unlabeled data
samples for which the model is least confident (i.e., most uncertain)
in prediction or class assignment.</p>
<p>It selects an instance <span class="arithmatex">\(x\)</span> such that</p>
<div class="arithmatex">\[\arg \max_{x} \; 1 - p(y_{max}|x, \theta)\]</div>
<p>where <span class="arithmatex">\(y_{max} = \arg\max_y p(y|x, \theta)\)</span>, i.e. the class label with the
highest posterior probability under the model <span class="arithmatex">\(\theta\)</span>. One way to interpret
this uncertainty measure is the expected 0/1-loss, i.e., the model's belief
that it will mislabel <span class="arithmatex">\(x\)</span>.</p>
<p>If samples from a posterior distributions are provided, it computes</p>
<div class="arithmatex">\[\arg \max_{x} \; 1 - \mathrm{E}_{p(\theta| D)} p(y_{max}|x, \theta)\]</div>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span>
<span class="normal"><a href="#__codelineno-0-178">178</a></span>
<span class="normal"><a href="#__codelineno-0-179">179</a></span>
<span class="normal"><a href="#__codelineno-0-180">180</a></span>
<span class="normal"><a href="#__codelineno-0-181">181</a></span>
<span class="normal"><a href="#__codelineno-0-182">182</a></span>
<span class="normal"><a href="#__codelineno-0-183">183</a></span>
<span class="normal"><a href="#__codelineno-0-184">184</a></span>
<span class="normal"><a href="#__codelineno-0-185">185</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-163" name="__codelineno-0-163"></a><span class="k">def</span> <span class="nf">least_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements the least confidence acquisition function.</span>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a><span class="sd">    References: http://burrsettles.com/pub/settles.activelearning.pdf.</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a><span class="sd">    This strategy allows an active learner to select the unlabeled data</span>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a><span class="sd">    samples for which the model is least confident (i.e., most uncertain)</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a><span class="sd">    in prediction or class assignment.</span>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a><span class="sd">    It selects an instance $x$ such that</span>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a><span class="sd">    $$\arg \max_{x} \; 1 - p(y_{max}|x, \theta)$$</span>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a><span class="sd">    where $y_{max} = \arg\max_y p(y|x, \theta)$, i.e. the class label with the</span>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a><span class="sd">    highest posterior probability under the model $\theta$. One way to interpret</span>
<a id="__codelineno-0-178" name="__codelineno-0-178"></a><span class="sd">    this uncertainty measure is the expected 0/1-loss, i.e., the model&#39;s belief</span>
<a id="__codelineno-0-179" name="__codelineno-0-179"></a><span class="sd">    that it will mislabel $x$.</span>
<a id="__codelineno-0-180" name="__codelineno-0-180"></a>
<a id="__codelineno-0-181" name="__codelineno-0-181"></a><span class="sd">    If samples from a posterior distributions are provided, it computes</span>
<a id="__codelineno-0-182" name="__codelineno-0-182"></a>
<a id="__codelineno-0-183" name="__codelineno-0-183"></a><span class="sd">    $$\arg \max_{x} \; 1 - \mathrm{E}_{p(\theta| D)} p(y_{max}|x, \theta)$$</span>
<a id="__codelineno-0-184" name="__codelineno-0-184"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-185" name="__codelineno-0-185"></a>    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.margin_confidence" class="doc doc-heading">
<code class="highlight language-python"><span class="n">margin_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.margin_confidence" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Implements the margin strategy.</p>
<p>Reference: http://burrsettles.com/pub/settles.activelearning.pdf.</p>
<p>Margin sampling aims to correct for a shortcoming in least
confident strategy, by incorporating the posterior of the second most likely label.
Intuitively, instances with large margins are easy, since the classifier has little
doubt in differentiating between the two most likely class labels. Instances with
small margins are more ambiguous, thus knowing the true label would help the
model discriminate more effectively between them.</p>
<p>It selects an instance <span class="arithmatex">\(x\)</span> such that</p>
<div class="arithmatex">\[\arg\min_{x} P(y_1|x, \theta) - P(y_2|x, \theta)\]</div>
<p>where <span class="arithmatex">\(y_1\)</span> and <span class="arithmatex">\(y_2\)</span> are the first and second most probable class labels under the
model defined by <span class="arithmatex">\(\theta\)</span>, respectively.</p>
<p>If samples from a posterior distributions are provided, it computes</p>
<div class="arithmatex">\[\arg\min_{x} \mathrm{E}_{p(\theta| D)} P(y_1|x, \theta) - \mathrm{E}_{p(\theta| D)} P(y_2|x, \theta)\]</div>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-213">213</a></span>
<span class="normal"><a href="#__codelineno-0-214">214</a></span>
<span class="normal"><a href="#__codelineno-0-215">215</a></span>
<span class="normal"><a href="#__codelineno-0-216">216</a></span>
<span class="normal"><a href="#__codelineno-0-217">217</a></span>
<span class="normal"><a href="#__codelineno-0-218">218</a></span>
<span class="normal"><a href="#__codelineno-0-219">219</a></span>
<span class="normal"><a href="#__codelineno-0-220">220</a></span>
<span class="normal"><a href="#__codelineno-0-221">221</a></span>
<span class="normal"><a href="#__codelineno-0-222">222</a></span>
<span class="normal"><a href="#__codelineno-0-223">223</a></span>
<span class="normal"><a href="#__codelineno-0-224">224</a></span>
<span class="normal"><a href="#__codelineno-0-225">225</a></span>
<span class="normal"><a href="#__codelineno-0-226">226</a></span>
<span class="normal"><a href="#__codelineno-0-227">227</a></span>
<span class="normal"><a href="#__codelineno-0-228">228</a></span>
<span class="normal"><a href="#__codelineno-0-229">229</a></span>
<span class="normal"><a href="#__codelineno-0-230">230</a></span>
<span class="normal"><a href="#__codelineno-0-231">231</a></span>
<span class="normal"><a href="#__codelineno-0-232">232</a></span>
<span class="normal"><a href="#__codelineno-0-233">233</a></span>
<span class="normal"><a href="#__codelineno-0-234">234</a></span>
<span class="normal"><a href="#__codelineno-0-235">235</a></span>
<span class="normal"><a href="#__codelineno-0-236">236</a></span>
<span class="normal"><a href="#__codelineno-0-237">237</a></span>
<span class="normal"><a href="#__codelineno-0-238">238</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-213" name="__codelineno-0-213"></a><span class="k">def</span> <span class="nf">margin_confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-214" name="__codelineno-0-214"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements the margin strategy.</span>
<a id="__codelineno-0-215" name="__codelineno-0-215"></a>
<a id="__codelineno-0-216" name="__codelineno-0-216"></a><span class="sd">    Reference: http://burrsettles.com/pub/settles.activelearning.pdf.</span>
<a id="__codelineno-0-217" name="__codelineno-0-217"></a>
<a id="__codelineno-0-218" name="__codelineno-0-218"></a><span class="sd">    Margin sampling aims to correct for a shortcoming in least</span>
<a id="__codelineno-0-219" name="__codelineno-0-219"></a><span class="sd">    confident strategy, by incorporating the posterior of the second most likely label.</span>
<a id="__codelineno-0-220" name="__codelineno-0-220"></a><span class="sd">    Intuitively, instances with large margins are easy, since the classifier has little</span>
<a id="__codelineno-0-221" name="__codelineno-0-221"></a><span class="sd">    doubt in differentiating between the two most likely class labels. Instances with</span>
<a id="__codelineno-0-222" name="__codelineno-0-222"></a><span class="sd">    small margins are more ambiguous, thus knowing the true label would help the</span>
<a id="__codelineno-0-223" name="__codelineno-0-223"></a><span class="sd">    model discriminate more effectively between them.</span>
<a id="__codelineno-0-224" name="__codelineno-0-224"></a>
<a id="__codelineno-0-225" name="__codelineno-0-225"></a><span class="sd">    It selects an instance $x$ such that</span>
<a id="__codelineno-0-226" name="__codelineno-0-226"></a>
<a id="__codelineno-0-227" name="__codelineno-0-227"></a><span class="sd">    $$\arg\min_{x} P(y_1|x, \theta) - P(y_2|x, \theta)$$</span>
<a id="__codelineno-0-228" name="__codelineno-0-228"></a>
<a id="__codelineno-0-229" name="__codelineno-0-229"></a><span class="sd">    where $y_1$ and $y_2$ are the first and second most probable class labels under the</span>
<a id="__codelineno-0-230" name="__codelineno-0-230"></a><span class="sd">    model defined by $\theta$, respectively.</span>
<a id="__codelineno-0-231" name="__codelineno-0-231"></a>
<a id="__codelineno-0-232" name="__codelineno-0-232"></a><span class="sd">    If samples from a posterior distributions are provided, it computes</span>
<a id="__codelineno-0-233" name="__codelineno-0-233"></a>
<a id="__codelineno-0-234" name="__codelineno-0-234"></a><span class="sd">    $$\arg\min_{x} \mathrm{E}_{p(\theta| D)} P(y_1|x, \theta) - \mathrm{E}_{p(\theta| D)} P(y_2|x, \theta)$$</span>
<a id="__codelineno-0-235" name="__codelineno-0-235"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-236" name="__codelineno-0-236"></a>    <span class="n">confidence_top2</span> <span class="o">=</span> <span class="n">confidence</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-237" name="__codelineno-0-237"></a>    <span class="c1"># we want the instances with the smallest gap, so we need to negate</span>
<a id="__codelineno-0-238" name="__codelineno-0-238"></a>    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">confidence_top2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">confidence_top2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="energizer.acquisition_functions.predictive_entropy" class="doc doc-heading">
<code class="highlight language-python"><span class="n">predictive_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></code>

<a href="#energizer.acquisition_functions.predictive_entropy" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  
      <p>Computes the predictive Shannon's entropy in nats.</p>
<p>It expects a tensor of logits with the following dimensions: <code>(B: batch_size, C: num_classes, S: num_inference_iterations)</code>.
This function implements the following steps, for each element along the <code>B: batch_size</code> dimension:</p>
<ul>
<li>
<p>Converts logits in probabilities along the <code>C: num_classes</code> dimension
<span class="arithmatex">\(<span class="arithmatex">\(p_{bcs} = e^{l_{bcs}} / \sum_j e^{l_{bjs}}\)</span>\)</span></p>
</li>
<li>
<p>Averages the output probabilities per class across samples
$$p_{bc} = \frac{1}{S} \sum_s p_{bcs}</p>
</li>
<li>
<p>Computes Shannon's entropy along the <code>C: num_classes</code> dimension
<span class="arithmatex">\(<span class="arithmatex">\(\mathrm{H}_{b}\left(\mathrm{p}(X) \right) = - \sum_c p_{bc} \log(p_{bc})\)</span>\)</span></p>
</li>
</ul>
<p>where <span class="arithmatex">\(l_{bcs}\)</span> is the logit for class <span class="arithmatex">\(c\)</span> for the <span class="arithmatex">\(b\)</span>-th element in the batch in the <span class="arithmatex">\(s\)</span>-th sample,
and <span class="arithmatex">\(\mathrm{p}\)</span> is a probability mass function for a random variable <span class="arithmatex">\(X\)</span> such that <span class="arithmatex">\(\mathrm{p}(X = c) = p_c\)</span>.</p>
<p>You can see the <code>entropy</code> function as a restriction of this in which we
only have one sample.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>logits</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>A tensor of dimensions <code>(B: batch_size, C: num_classes, S: num_inference_iterations)</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The Shannon's entropy, i.e. a vector of dimensions <code>(B: batch_size,)</code>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>energizer/acquisition_functions.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-33">33</a></span>
<span class="normal"><a href="#__codelineno-0-34">34</a></span>
<span class="normal"><a href="#__codelineno-0-35">35</a></span>
<span class="normal"><a href="#__codelineno-0-36">36</a></span>
<span class="normal"><a href="#__codelineno-0-37">37</a></span>
<span class="normal"><a href="#__codelineno-0-38">38</a></span>
<span class="normal"><a href="#__codelineno-0-39">39</a></span>
<span class="normal"><a href="#__codelineno-0-40">40</a></span>
<span class="normal"><a href="#__codelineno-0-41">41</a></span>
<span class="normal"><a href="#__codelineno-0-42">42</a></span>
<span class="normal"><a href="#__codelineno-0-43">43</a></span>
<span class="normal"><a href="#__codelineno-0-44">44</a></span>
<span class="normal"><a href="#__codelineno-0-45">45</a></span>
<span class="normal"><a href="#__codelineno-0-46">46</a></span>
<span class="normal"><a href="#__codelineno-0-47">47</a></span>
<span class="normal"><a href="#__codelineno-0-48">48</a></span>
<span class="normal"><a href="#__codelineno-0-49">49</a></span>
<span class="normal"><a href="#__codelineno-0-50">50</a></span>
<span class="normal"><a href="#__codelineno-0-51">51</a></span>
<span class="normal"><a href="#__codelineno-0-52">52</a></span>
<span class="normal"><a href="#__codelineno-0-53">53</a></span>
<span class="normal"><a href="#__codelineno-0-54">54</a></span>
<span class="normal"><a href="#__codelineno-0-55">55</a></span>
<span class="normal"><a href="#__codelineno-0-56">56</a></span>
<span class="normal"><a href="#__codelineno-0-57">57</a></span>
<span class="normal"><a href="#__codelineno-0-58">58</a></span>
<span class="normal"><a href="#__codelineno-0-59">59</a></span>
<span class="normal"><a href="#__codelineno-0-60">60</a></span>
<span class="normal"><a href="#__codelineno-0-61">61</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-33" name="__codelineno-0-33"></a><span class="k">def</span> <span class="nf">predictive_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a>    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the predictive Shannon&#39;s entropy in nats.</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a><span class="sd">    It expects a tensor of logits with the following dimensions: `(B: batch_size, C: num_classes, S: num_inference_iterations)`.</span>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a><span class="sd">    This function implements the following steps, for each element along the `B: batch_size` dimension:</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a><span class="sd">    - Converts logits in probabilities along the `C: num_classes` dimension</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a><span class="sd">    $$p_{bcs} = e^{l_{bcs}} / \sum_j e^{l_{bjs}}$$</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a><span class="sd">    - Averages the output probabilities per class across samples</span>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a><span class="sd">    $$p_{bc} = \frac{1}{S} \sum_s p_{bcs}</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a><span class="sd">    - Computes Shannon&#39;s entropy along the `C: num_classes` dimension</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a><span class="sd">    $$\mathrm{H}_{b}\left(\mathrm{p}(X) \right) = - \sum_c p_{bc} \log(p_{bc})$$</span>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a><span class="sd">    where $l_{bcs}$ is the logit for class $c$ for the $b$-th element in the batch in the $s$-th sample,</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a><span class="sd">    and $\mathrm{p}$ is a probability mass function for a random variable $X$ such that $\mathrm{p}(X = c) = p_c$.</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a><span class="sd">    You can see the `entropy` function as a restriction of this in which we</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a><span class="sd">    only have one sample.</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a><span class="sd">        logits (Tensor): A tensor of dimensions `(B: batch_size, C: num_classes, S: num_inference_iterations)`.</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a><span class="sd">        The Shannon&#39;s entropy, i.e. a vector of dimensions `(B: batch_size,)`.</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a>    <span class="n">avg_probs</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a>    <span class="k">return</span> <span class="n">entr</span><span class="p">(</span><span class="n">avg_probs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">September 11, 2022</span>
      
        <br>
        Created:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">September 11, 2022</span>
      
    
  </small>
</div>




              
            </article>
            
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../query_strategies/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Query strategies" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Query strategies
            </div>
          </div>
        </a>
      
      
        
        <a href="../../contributing/" class="md-footer__link md-footer__link--next" aria-label="Next: Contributing" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Contributing
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - Pietro Lesci
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    <a href="https://twitter.com/pietro_lesci" target="_blank" rel="noopener" title="Twitter" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    <a href="https://www.linkedin.com/in/pietrolesci" target="_blank" rel="noopener" title="LinkedIn" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
    <a href="https://github.com/pietrolesci/" target="_blank" rel="noopener" title="Github" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    <a href="mailto:pietrolesci@outlook.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2Z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.sections", "navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "search.suggest", "search.highlight", "search.share", "header.autohide", "announce.dismiss"], "search": "../../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": {"default": "stable", "provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.2ab50757.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>