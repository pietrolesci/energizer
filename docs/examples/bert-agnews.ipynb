{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "trainer_kwargs = {\n",
    "    \"query_size\": 1,\n",
    "    \"max_epochs\": 3,\n",
    "    \"max_labelling_epochs\": 5,\n",
    "    \"test_after_labelling\": True,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"limit_val_batches\": 1,\n",
    "    # total_budget=5,\n",
    "    # for testing purposes\n",
    "    # limit_train_batches=10,\n",
    "    # limit_test_batches=10,\n",
    "    # limit_pool_batches=10,\n",
    "    # log_every_n_steps=1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer as PLTrainer\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy, F1Score, MetricCollection, Precision, Recall\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_constant_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from energizer import Trainer\n",
    "from energizer.acquisition_functions import entropy, expected_entropy\n",
    "from energizer.data.datamodule import ActiveDataModuleWithIndex\n",
    "from energizer.query_strategies.strategies import RandomArchorPointsStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ag_news (/home/pl487/.cache/huggingface/datasets/pietrolesci___ag_news/concat/1.0.0/5ee6e111adc7a901ca734b79fbebff09d9dba91722387a794efff8d9c178a6a3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061a9f6e12c44dd79a1986c2779f887f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413c7d7685554594a08539b6c26c4441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb46e4962f974881bf9fe208a9aed57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# renames \"label\" to \"labels\"\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(\"pietrolesci/ag_news\", \"concat\")\n",
    "\n",
    "# tokenize\n",
    "dataset = dataset.map(lambda ex: tokenizer(ex[\"text\"]), batched=True)\n",
    "columns_to_keep = [\"label\", \"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "\n",
    "# train-val split and record datasets\n",
    "train_set, test_set = dataset[\"train\"], dataset[\"test\"]\n",
    "_split = train_set.train_test_split(0.3)\n",
    "_, val_set = _split[\"train\"], _split[\"test\"]\n",
    "\n",
    "labels = train_set.features[\"label\"].names\n",
    "num_classes = len(labels)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = BATCH_SIZE\n",
    "eval_batch_size = EVAL_BATCH_SIZE  # this is use when evaluating on the pool too\n",
    "train_dl = DataLoader(\n",
    "    train_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_classes: int,\n",
    "        learning_rate: float = 0.00001,\n",
    "        num_warmup_steps: int = 50,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_classes,\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        for stage in (\"train\", \"val\", \"test\"):\n",
    "            metrics = MetricCollection(\n",
    "                {\n",
    "                    \"accuracy\": Accuracy(),\n",
    "                    \"precision_macro\": Precision(\n",
    "                        num_classes=num_classes, average=\"macro\"\n",
    "                    ),\n",
    "                    \"recall_macro\": Recall(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"f1_macro\": F1Score(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"f1_micro\": F1Score(num_classes=num_classes, average=\"micro\"),\n",
    "                }\n",
    "            )\n",
    "            setattr(self, f\"{stage}_metrics\", metrics)\n",
    "\n",
    "    def common_step(self, batch: Any, stage: str):\n",
    "        \"\"\"Outputs loss and logits, logs loss and metrics.\"\"\"\n",
    "        out = self(batch)\n",
    "        logits, loss = out.logits, out.loss\n",
    "        self.log(f\"{stage}/loss\", loss)\n",
    "\n",
    "        metrics = getattr(self, f\"{stage}_metrics\")(logits, batch[\"labels\"])\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, batch) -> torch.Tensor:\n",
    "        return self.model(**batch)\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Any, batch_idx: int = 0, optimizer_idx: int = 0\n",
    "    ) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        optimizer = AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.learning_rate,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": get_constant_schedule_with_warmup(\n",
    "                    optimizer=optimizer, num_warmup_steps=self.num_warmup_steps\n",
    "                ),\n",
    "                \"monitor\": \"val/loss\",\n",
    "                \"frequency\": 1,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(\n",
    "    model_name=MODEL_NAME, num_classes=num_classes, learning_rate=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_strategy = RandomStrategy(deepcopy(model))\n",
    "\n",
    "seed_everything(1994)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=random_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")\n",
    "random_df = results.to_pandas()\n",
    "random_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AccumulatorStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyStrategy(AccumulatorStrategy):\n",
    "    \"\"\"A implememntation of the `Entropy` active learning strategy.\"\"\"\n",
    "\n",
    "    def get_inputs_from_batch(self, batch: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
    "        batch.pop(\"labels\")\n",
    "        return batch\n",
    "\n",
    "    def pool_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Tensor:\n",
    "        logits = self(batch).logits\n",
    "        return entropy(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_strategy = EntropyStrategy(deepcopy(model))\n",
    "\n",
    "seed_everything(1994)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=entropy_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")\n",
    "entropy_df = results.to_pandas()\n",
    "entropy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnchorPointsStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRandomArchorPointsStrategy(RandomArchorPointsStrategy):\n",
    "    def get_search_query_from_batch(self, batch: Any) -> Tensor:\n",
    "        return batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1994\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[1;32m[2022-09-11 23:15:10] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:269\u001b[0m$ \u001b[37mTrainer: trainer active_fit stage\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name          | Type                          | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model         | BertForSequenceClassification | 4.4 M \n",
      "1 | train_metrics | MetricCollection              | 0     \n",
      "2 | val_metrics   | MetricCollection              | 0     \n",
      "3 | test_metrics  | MetricCollection              | 0     \n",
      "----------------------------------------------------------------\n",
      "4.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.546    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9692ed1cff0942899919763bfee64b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/core/module.py:555: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=self.device)\n",
      "\u001b[1;36m[2022-09-11 23:15:11] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:464\u001b[0m$ \u001b[37mUsing `RandomArchorPointsStrategy`\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 0--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:11] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a640718f8444a78ba94be3c9a0e85a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.19736842811107635    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.12158314883708954    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.19736842811107635    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_macro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10476590692996979    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1972827911376953     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4013481140136719     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.19736842811107635   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12158314883708954   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.19736842811107635   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_macro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10476590692996979   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1972827911376953    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4013481140136719    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:12] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:464\u001b[0m$ \u001b[37mUsing `RandomArchorPointsStrategy`\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, pool_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[1;32m[2022-09-11 23:15:12] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:193\u001b[0m$ \u001b[37mQueried 1 instance\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:12] energizer/DEBUG\u001b[0m ~ \u001b[1;33mdatamodule:322\u001b[0m$ \u001b[37mUpdating `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:12] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:281\u001b[0m$ \u001b[37mAnnotated 1 instances\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:12] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:282\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 235\n",
      "num_train_batches: 1\n",
      "pool_size: 119999\n",
      "total_data_size: 120000\n",
      "train_size: 1\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 1--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:12] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:12] energizer/DEBUG\u001b[0m ~ \u001b[1;33mactive_learning_loop:250\u001b[0m$ \u001b[37mTransformerModel state dict has been re-initialized\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f584877a13446fb2d928ebbb0b3a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f842eab638c4cd28f613e6bd6448ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1678d2696b93484c8efb2ff155f1efef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415ce9c81cb9481c82f68e1b8724b40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;36m[2022-09-11 23:15:15] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b2d994cd3b4ddd8e7547ac7b23b4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.19684210419654846    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.12209660559892654    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.19684210419654846    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_macro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10464777797460556    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.19679869711399078    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.401223063468933     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.19684210419654846   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12209660559892654   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.19684210419654846   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_macro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10464777797460556   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.19679869711399078   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.401223063468933    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:16] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:464\u001b[0m$ \u001b[37mUsing `RandomArchorPointsStrategy`\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:16] energizer/DEBUG\u001b[0m ~ \u001b[1;33mdatamodule:306\u001b[0m$ \u001b[37mSearching `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:16] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:193\u001b[0m$ \u001b[37mQueried 1 instance\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:16] energizer/DEBUG\u001b[0m ~ \u001b[1;33mdatamodule:322\u001b[0m$ \u001b[37mUpdating `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:16] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:281\u001b[0m$ \u001b[37mAnnotated 1 instances\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:16] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:282\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 235\n",
      "num_train_batches: 1\n",
      "pool_size: 119998\n",
      "total_data_size: 120000\n",
      "train_size: 2\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 2--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:16] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:16] energizer/DEBUG\u001b[0m ~ \u001b[1;33mactive_learning_loop:250\u001b[0m$ \u001b[37mTransformerModel state dict has been re-initialized\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb23ee1ad2d43589c4b7e78070cfb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd5e3b8a29e46bcb1742be1eb62e6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb57f3012476483884178a9481340494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb6e71fff184daebc389cf8478aeada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;36m[2022-09-11 23:15:19] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86868ae103d7424ebbd90fb932318927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.19842104613780975    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.12175324559211731    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.19842104613780975    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_macro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10508212447166443    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.19832772016525269    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.401254653930664     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.19842104613780975   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12175324559211731   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.19842104613780975   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_macro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10508212447166443   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.19832772016525269   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.401254653930664    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:20] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:464\u001b[0m$ \u001b[37mUsing `RandomArchorPointsStrategy`\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:20] energizer/DEBUG\u001b[0m ~ \u001b[1;33mdatamodule:306\u001b[0m$ \u001b[37mSearching `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:20] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:193\u001b[0m$ \u001b[37mQueried 1 instance\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:20] energizer/DEBUG\u001b[0m ~ \u001b[1;33mdatamodule:322\u001b[0m$ \u001b[37mUpdating `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:20] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:281\u001b[0m$ \u001b[37mAnnotated 1 instances\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:20] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:282\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 235\n",
      "num_train_batches: 1\n",
      "pool_size: 119997\n",
      "total_data_size: 120000\n",
      "train_size: 3\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 3--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:20] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:20] energizer/DEBUG\u001b[0m ~ \u001b[1;33mactive_learning_loop:250\u001b[0m$ \u001b[37mTransformerModel state dict has been re-initialized\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a546e5db60c419d930ea6e7be28736b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10458b5e53942bbb9f61fcab79efb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f908602b9a4258af34ee3aabb7295b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a28f6101fc1484cbd72f119dc0eb492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;36m[2022-09-11 23:15:23] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711905504abe419b8ffd89dce66015e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20197369158267975    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1201840341091156     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20197369158267975    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_macro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1055801585316658     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2016625702381134     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4014556407928467     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20197369158267975   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1201840341091156    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20197369158267975   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_macro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1055801585316658    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2016625702381134    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4014556407928467    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:24] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:464\u001b[0m$ \u001b[37mUsing `RandomArchorPointsStrategy`\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:24] energizer/DEBUG\u001b[0m ~ \u001b[1;33mdatamodule:306\u001b[0m$ \u001b[37mSearching `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:24] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:193\u001b[0m$ \u001b[37mQueried 1 instance\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:24] energizer/DEBUG\u001b[0m ~ \u001b[1;33mdatamodule:322\u001b[0m$ \u001b[37mUpdating `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:25] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:281\u001b[0m$ \u001b[37mAnnotated 1 instances\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:25] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:282\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 235\n",
      "num_train_batches: 1\n",
      "pool_size: 119996\n",
      "total_data_size: 120000\n",
      "train_size: 4\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 4--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:25] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:25] energizer/DEBUG\u001b[0m ~ \u001b[1;33mactive_learning_loop:250\u001b[0m$ \u001b[37mTransformerModel state dict has been re-initialized\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ada588d3e774fe5833bb419183775e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1876e6d1f44695b22b183f5c6bf547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2711e8aab8540e7982855b9ac408d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02abe1b4c25d4417bd2ee345cb249df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;36m[2022-09-11 23:15:27] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d121d6606774ce495a33e30f9b670e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20855262875556946    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1171206682920456     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20855262875556946    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_macro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10607849061489105    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20835711061954498    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4018421173095703     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20855262875556946   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1171206682920456    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20855262875556946   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_macro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10607849061489105   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20835711061954498   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4018421173095703    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:28] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:464\u001b[0m$ \u001b[37mUsing `RandomArchorPointsStrategy`\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:28] energizer/DEBUG\u001b[0m ~ \u001b[1;33mdatamodule:306\u001b[0m$ \u001b[37mSearching `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:29] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:193\u001b[0m$ \u001b[37mQueried 1 instance\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:29] energizer/DEBUG\u001b[0m ~ \u001b[1;33mdatamodule:322\u001b[0m$ \u001b[37mUpdating `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:29] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:281\u001b[0m$ \u001b[37mAnnotated 1 instances\u001b[0m\n",
      "\u001b[1;32m[2022-09-11 23:15:29] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:282\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 235\n",
      "num_train_batches: 1\n",
      "pool_size: 119995\n",
      "total_data_size: 120000\n",
      "train_size: 5\n",
      "\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:29] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n",
      "\u001b[1;36m[2022-09-11 23:15:29] energizer/DEBUG\u001b[0m ~ \u001b[1;33mactive_learning_loop:250\u001b[0m$ \u001b[37mTransformerModel state dict has been re-initialized\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Last fit_loop------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62392121e654e7a871e3186aaf6f3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8978f009e0e04bd7b29deba314508276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0635370be603489c93b938779b109df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e8cf0fa38d4174868965f7503061e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;36m[2022-09-11 23:15:32] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:468\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5be8bb1329145629731512d73dba32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2139473706483841     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11554442346096039    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2139473706483841     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_macro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10658220201730728    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.21365869045257568    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4021679162979126     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2139473706483841    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11554442346096039   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2139473706483841    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_macro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10658220201730728   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.21365869045257568   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4021679162979126    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2022-09-11 23:15:33] energizer/DEBUG\u001b[0m ~ \u001b[1;33mtrainer:464\u001b[0m$ \u001b[37mUsing `RandomArchorPointsStrategy`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>test/loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.401348</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.121583</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.104766</td>\n",
       "      <td>0.197283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.401223</td>\n",
       "      <td>0.196842</td>\n",
       "      <td>0.122097</td>\n",
       "      <td>0.196842</td>\n",
       "      <td>0.104648</td>\n",
       "      <td>0.196799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.401255</td>\n",
       "      <td>0.198421</td>\n",
       "      <td>0.121753</td>\n",
       "      <td>0.198421</td>\n",
       "      <td>0.105082</td>\n",
       "      <td>0.198328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.401456</td>\n",
       "      <td>0.201974</td>\n",
       "      <td>0.120184</td>\n",
       "      <td>0.201974</td>\n",
       "      <td>0.105580</td>\n",
       "      <td>0.201663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.401842</td>\n",
       "      <td>0.208553</td>\n",
       "      <td>0.117121</td>\n",
       "      <td>0.208553</td>\n",
       "      <td>0.106078</td>\n",
       "      <td>0.208357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.402168</td>\n",
       "      <td>0.213947</td>\n",
       "      <td>0.115544</td>\n",
       "      <td>0.213947</td>\n",
       "      <td>0.106582</td>\n",
       "      <td>0.213659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_size  test/loss  accuracy  ...  f1_micro  precision_macro  recall_macro\n",
       "0           0   1.401348  0.197368  ...  0.197368         0.104766      0.197283\n",
       "1           1   1.401223  0.196842  ...  0.196842         0.104648      0.196799\n",
       "2           2   1.401255  0.198421  ...  0.198421         0.105082      0.198328\n",
       "3           3   1.401456  0.201974  ...  0.201974         0.105580      0.201663\n",
       "4           4   1.401842  0.208553  ...  0.208553         0.106078      0.208357\n",
       "5           5   1.402168  0.213947  ...  0.213947         0.106582      0.213659\n",
       "\n",
       "[6 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_anchor_points_strategy = RandomArchorPointsStrategy(deepcopy(model), 10)\n",
    "\n",
    "datamodule = ActiveDataModuleWithIndex(\n",
    "    train_dataloader=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    "    faiss_index_path=\"all-mpnet-base-v2_ag-news_train.faiss\",\n",
    ")\n",
    "\n",
    "seed_everything(1994)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=random_anchor_points_strategy,\n",
    "    datamodule=datamodule,\n",
    ")\n",
    "rap_df = results.to_pandas()\n",
    "rap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(random_df[\"train_size\"], random_df[\"accuracy\"], label=\"random\")\n",
    "plt.plot(entropy_df[\"train_size\"], entropy_df[\"accuracy\"], label=\"entropy\")\n",
    "plt.plot(rap_df[\"train_size\"], rap_df[\"accuracy\"], label=\"random anchors\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df[\"strategy\"] = \"random\"\n",
    "entropy_df[\"strategy\"] = \"entropy\"\n",
    "rap_df[\"strategy\"] = \"random_anchors\"\n",
    "results = pd.concat([random_df, entropy_df, rap_df], ignore_index=False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.to_parquet(\"results_al.parquet\", index=False)\n",
    "# with open(\"results_al_metadata.json\", \"w\") as fl:\n",
    "#     json.dump(trainer_kwargs, fl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('energizer-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf3d60d746ce6794d4ec556d1628bdd1ecace636e1760c52a7757d10f6e042be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
