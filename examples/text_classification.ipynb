{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer as PLTrainer\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy, F1Score, MetricCollection, Precision, Recall\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_constant_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from energizer import AccumulatorStrategy, RandomStrategy, Trainer\n",
    "from energizer.acquisition_functions import entropy, expected_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# renames \"label\" to \"labels\"\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(\"pietrolesci/ag_news\", \"concat\")\n",
    "\n",
    "# tokenize\n",
    "dataset = dataset.map(lambda ex: tokenizer(ex[\"text\"]), batched=True)\n",
    "columns_to_keep = [\"label\", \"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "\n",
    "# train-val split and record datasets\n",
    "train_set, test_set = dataset[\"train\"], dataset[\"test\"]\n",
    "_split = train_set.train_test_split(0.3)\n",
    "train_set, val_set = _split[\"train\"], _split[\"test\"]\n",
    "\n",
    "labels = train_set.features[\"label\"].names\n",
    "num_classes = len(labels)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = BATCH_SIZE\n",
    "eval_batch_size = EVAL_BATCH_SIZE  # this is use when evaluating on the pool too\n",
    "train_dl = DataLoader(\n",
    "    train_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_classes: int,\n",
    "        learning_rate: float = 0.00001,\n",
    "        num_warmup_steps: int = 50,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_classes,\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        for stage in (\"train\", \"val\", \"test\"):\n",
    "            metrics = MetricCollection(\n",
    "                {\n",
    "                    \"accuracy\": Accuracy(),\n",
    "                    \"precision_macro\": Precision(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"precision_micro\": Precision(num_classes=num_classes, average=\"micro\"),\n",
    "                    \"recall_macro\": Recall(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"recall_micro\": Recall(num_classes=num_classes, average=\"micro\"),\n",
    "                    \"f1_macro\": F1Score(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"f1_micro\": F1Score(num_classes=num_classes, average=\"micro\"),\n",
    "                }\n",
    "            )\n",
    "            setattr(self, f\"{stage}_metrics\", metrics)\n",
    "\n",
    "    def common_step(self, batch: Any, stage: str):\n",
    "        \"\"\"Outputs loss and logits, logs loss and metrics.\"\"\"\n",
    "        out = self(batch)\n",
    "        logits, loss = out.logits, out.loss\n",
    "        self.log(f\"{stage}/loss\", loss)\n",
    "\n",
    "        metrics = getattr(self, f\"{stage}_metrics\")(logits, batch[\"labels\"])\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, batch) -> torch.Tensor:\n",
    "        return self.model(**batch)\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int = 0, optimizer_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        optimizer = AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.learning_rate)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": get_constant_schedule_with_warmup(\n",
    "                    optimizer=optimizer, num_warmup_steps=self.num_warmup_steps\n",
    "                ),\n",
    "                \"monitor\": \"val/loss\",\n",
    "                \"frequency\": 1,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyStrategy(AccumulatorStrategy):\n",
    "    \"\"\"A implememntation of the `Entropy` active learning strategy.\"\"\"\n",
    "\n",
    "    def pool_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Tensor:\n",
    "        batch.pop(\"labels\")\n",
    "        logits = self(batch).logits\n",
    "        return entropy(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_strategy = EntropyStrategy(deepcopy(model))\n",
    "# random_strategy = RandomStrategy(deepcopy(model))\n",
    "\n",
    "# batch = next(iter(train_dl))\n",
    "# model(batch).logits.shape, entropy_strategy(batch).logits.shape, random_strategy(batch).logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(model_name=MODEL_NAME, num_classes=num_classes, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_strategy = RandomStrategy(deepcopy(model))\n",
    "\n",
    "seed_everything(1994)\n",
    "\n",
    "trainer = Trainer(\n",
    "    query_size=50,\n",
    "    max_epochs=3,\n",
    "    max_labelling_epochs=10,\n",
    "    # total_budget=5,\n",
    "    test_after_labelling=True,\n",
    "    accelerator=\"gpu\",\n",
    "    # for testing purposes\n",
    "    # limit_train_batches=10,\n",
    "    limit_val_batches=1,\n",
    "    # limit_test_batches=10,\n",
    "    # limit_pool_batches=10,\n",
    "    # log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "results = trainer.active_fit(\n",
    "    model=random_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = pd.DataFrame(\n",
    "    data=[(l.data_stats[\"train_size\"], *l.test_outputs[0].values()) for l in results],\n",
    "    columns=(\"train_size\", *results[0].test_outputs[0].keys()),\n",
    ")\n",
    "random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_strategy = EntropyStrategy(deepcopy(model))\n",
    "\n",
    "seed_everything(1994)\n",
    "\n",
    "trainer = Trainer(\n",
    "    query_size=50,\n",
    "    max_epochs=3,\n",
    "    max_labelling_epochs=2,\n",
    "    # total_budget=5,\n",
    "    test_after_labelling=True,\n",
    "    accelerator=\"gpu\",\n",
    "    # for testing purposes\n",
    "    # limit_train_batches=10,\n",
    "    limit_val_batches=1,\n",
    "    # limit_test_batches=10,\n",
    "    # limit_pool_batches=10,\n",
    "    # log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "results = trainer.active_fit(\n",
    "    model=entropy_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_df = pd.DataFrame(\n",
    "    data=[(l.data_stats[\"train_size\"], *l.test_outputs[0].values()) for l in results],\n",
    "    columns=(\"train_size\", *results[0].test_outputs[0].keys()),\n",
    ")\n",
    "entropy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('energizer-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf3d60d746ce6794d4ec556d1628bdd1ecace636e1760c52a7757d10f6e042be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
