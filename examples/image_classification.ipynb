{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer as PLTrainer\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy, F1Score, MetricCollection, Precision, Recall\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from energizer import AccumulatorStrategy, RandomStrategy, Trainer\n",
    "from energizer.acquisition_functions import entropy, expected_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1312804/2032656520.py\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m datamodule = ActiveDataModuleWithIndex(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/energizer/energizer/data/datamodule.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_dataloader, val_dataloaders, test_dataloaders, datamodule, faiss_index_path)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaiss_index_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss_index_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_faiss_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaiss_index_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_faiss_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load and preprocess datasets\n",
    "data_dir = \"./data\"\n",
    "preprocessing_pipe = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "train_set = MNIST(data_dir, train=True, download=True, transform=preprocessing_pipe)\n",
    "test_set = MNIST(data_dir, train=False, download=True, transform=preprocessing_pipe)\n",
    "train_set, val_set = random_split(train_set, [55000, 5000])\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 32\n",
    "eval_batch_size = 128  # this is use when evaluating on the pool too\n",
    "train_dl = DataLoader(train_set, batch_size=batch_size)\n",
    "val_dl = DataLoader(val_set, batch_size=eval_batch_size)\n",
    "test_dl = DataLoader(test_set, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "        for stage in (\"train\", \"val\", \"test\"):\n",
    "            setattr(self, f\"{stage}_accuracy\", Accuracy())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def loss(self, logits: Tensor, targets: Tensor) -> Tensor:\n",
    "        return F.cross_entropy(logits, targets)\n",
    "\n",
    "    def step(self, batch: Tuple[Tensor, Tensor], stage: str) -> Dict[str, Tensor]:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        accuracy = getattr(self, f\"{stage}_accuracy\")(logits, y)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(f\"{stage}/accuracy\", accuracy, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        return self.step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self) -> None:\n",
    "        return torch.optim.SGD(self.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyStrategy(AccumulatorStrategy):\n",
    "    \"\"\"A implememntation of the `Entropy` active learning strategy.\"\"\"\n",
    "\n",
    "    def pool_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Tensor:\n",
    "        # define how to perform the forward pass\n",
    "        x, _ = batch\n",
    "        logits = self(x)\n",
    "        # use an acquisition/scoring function\n",
    "        scores = entropy(logits)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 10]), torch.Size([32, 10]), torch.Size([32, 10]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MNISTModel()\n",
    "entropy_strategy = EntropyStrategy(model)\n",
    "random_strategy = RandomStrategy(model)\n",
    "\n",
    "x, _ = next(iter(train_dl))\n",
    "model(x).shape, entropy_strategy(x).shape, random_strategy(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[1;36m[2022-08-27 14:36:30] energizer/DETAIL\u001b[0m ~ \u001b[1;33mtrainer:227\u001b[0m$ Trainer: trainer active_fit stage\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | model          | Sequential | 184 K \n",
      "1 | train_accuracy | Accuracy   | 0     \n",
      "2 | val_accuracy   | Accuracy   | 0     \n",
      "3 | test_accuracy  | Accuracy   | 0     \n",
      "----------------------------------------------\n",
      "184 K     Trainable params\n",
      "0         Non-trainable params\n",
      "184 K     Total params\n",
      "0.738     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b040dd4a41242e38ec176ecb62df7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[1;32m[2022-08-27 14:36:33] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `RandomStrategy`\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 0--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:33] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec55733102e04cc39ddc22243f533e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test/accuracy_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.07540000230073929    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.327603578567505     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test/accuracy_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.07540000230073929   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.327603578567505    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:35] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `RandomStrategy`\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, pool_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[1;32m[2022-08-27 14:36:35] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:195\u001b[0m$ \u001b[37mQueried 100 instance\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:35] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:283\u001b[0m$ \u001b[37mAnnotated 100 instances.\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:35] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:284\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 429\n",
      "num_train_batches: 4\n",
      "pool_size: 54900\n",
      "total_data_size: 55000\n",
      "train_size: 100\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 1--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:35] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:35] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:252\u001b[0m$ \u001b[37mMNISTModel state dict has been re-initialized\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168fef3d924f49e396a408e796bf20d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba1e99b559646debf8860f3a22da550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63efc9ab0d44adabaab9b98384a68f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fb9621a5a043498625e466fb1dca41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;32m[2022-08-27 14:36:36] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ce2c11f56b4eeeb4045e1225dcd2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test/accuracy_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2223999947309494     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.2602434158325195     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test/accuracy_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2223999947309494    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.2602434158325195    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:38] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `RandomStrategy`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:38] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:195\u001b[0m$ \u001b[37mQueried 100 instance\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:38] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:283\u001b[0m$ \u001b[37mAnnotated 100 instances.\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:38] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:284\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 429\n",
      "num_train_batches: 7\n",
      "pool_size: 54800\n",
      "total_data_size: 55000\n",
      "train_size: 200\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 2--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:38] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:38] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:252\u001b[0m$ \u001b[37mMNISTModel state dict has been re-initialized\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d31566f831947c7ae128f9de317da49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 4it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf78dd24470c4e208ebf7070bd9c7607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cedde15a7c43c8ab24ec370fd2c08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4159385b7fa4d40a6afaf7272b61813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;32m[2022-08-27 14:36:38] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047695555eb04983903de6a83917710f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test/accuracy_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.17649999260902405    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.235868453979492     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test/accuracy_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.17649999260902405   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.235868453979492    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:40] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `RandomStrategy`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:40] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:195\u001b[0m$ \u001b[37mQueried 100 instance\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:40] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:283\u001b[0m$ \u001b[37mAnnotated 100 instances.\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:40] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:284\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 428\n",
      "num_train_batches: 10\n",
      "pool_size: 54700\n",
      "total_data_size: 55000\n",
      "train_size: 300\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 3--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:40] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:40] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:252\u001b[0m$ \u001b[37mMNISTModel state dict has been re-initialized\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445c0b5eb94e407a9635593dcef1cf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0982035a4141eaa832f73b25034738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72034f1d8e0f4bbd989831c6d206b36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615f4183d5f24a6d946a8e414084c012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;32m[2022-08-27 14:36:41] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185a401104ee4e34ae85a3566612196d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test/accuracy_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.24500000476837158    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.188534736633301     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test/accuracy_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.24500000476837158   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.188534736633301    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:43] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `RandomStrategy`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:43] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:195\u001b[0m$ \u001b[37mQueried 100 instance\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:43] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:283\u001b[0m$ \u001b[37mAnnotated 100 instances.\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:43] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:284\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 427\n",
      "num_train_batches: 13\n",
      "pool_size: 54600\n",
      "total_data_size: 55000\n",
      "train_size: 400\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 4--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:43] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:43] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:252\u001b[0m$ \u001b[37mMNISTModel state dict has been re-initialized\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579315e1dc0e47349a992f886a8a66b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 10it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238a7144364246be927ad5534062e97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254d480f6de34262895442856472ee02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9738f693ec43139cb5398dce20d42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;32m[2022-08-27 14:36:44] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b02439b55e44dc948b020f4cf4aa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test/accuracy_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4122999906539917     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.1419079303741455     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test/accuracy_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4122999906539917    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.1419079303741455    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:46] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `RandomStrategy`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:46] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:195\u001b[0m$ \u001b[37mQueried 100 instance\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:46] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:283\u001b[0m$ \u001b[37mAnnotated 100 instances.\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:46] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:284\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 426\n",
      "num_train_batches: 16\n",
      "pool_size: 54500\n",
      "total_data_size: 55000\n",
      "train_size: 500\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Last fit_loop------------------------------\n",
      "-------------------------Labelling Iteration 5--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:46] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 14:36:46] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:252\u001b[0m$ \u001b[37mMNISTModel state dict has been re-initialized\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e2691260764ed1842e71392a8d64ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 13it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84f7ad1f0b34e068de9be1d3da57c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032bbf00f9e1432a91de442400b814a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f9ca69aa094847a858d763094aaae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;32m[2022-08-27 14:36:47] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `MNISTModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7da532956d461098659f20710bd907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test/accuracy_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5321999788284302     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.096198797225952     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test/accuracy_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5321999788284302    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.096198797225952    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 14:36:49] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `RandomStrategy`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "random_strategy = RandomStrategy(deepcopy(model))\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    query_size=100,\n",
    "    max_epochs=3,\n",
    "    max_labelling_epochs=5,\n",
    "    accelerator=\"gpu\",\n",
    "    # total_budget=5,\n",
    "    test_after_labelling=True,\n",
    "    # for testing purposes\n",
    "    # limit_train_batches=10,\n",
    "    limit_val_batches=1,\n",
    "    # limit_test_batches=10,\n",
    "    # limit_pool_batches=10,\n",
    "    # log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "results = trainer.active_fit(\n",
    "    model=random_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>test/loss_epoch</th>\n",
       "      <th>test/accuracy_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.327604</td>\n",
       "      <td>0.0754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>2.260243</td>\n",
       "      <td>0.2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>2.235868</td>\n",
       "      <td>0.1765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>2.188535</td>\n",
       "      <td>0.2450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>2.141908</td>\n",
       "      <td>0.4123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>2.096199</td>\n",
       "      <td>0.5322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_size  test/loss_epoch  test/accuracy_epoch\n",
       "0           0         2.327604               0.0754\n",
       "1         100         2.260243               0.2224\n",
       "2         200         2.235868               0.1765\n",
       "3         300         2.188535               0.2450\n",
       "4         400         2.141908               0.4123\n",
       "5         500         2.096199               0.5322"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_df = results.to_pandas()\n",
    "random_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_strategy = EntropyStrategy(deepcopy(model))\n",
    "\n",
    "trainer = Trainer(\n",
    "    query_size=100,\n",
    "    max_epochs=3,\n",
    "    max_labelling_epochs=5,\n",
    "    accelerator=\"gpu\",\n",
    "    # total_budget=5,\n",
    "    test_after_labelling=True,\n",
    "    # for testing purposes\n",
    "    # limit_train_batches=10,\n",
    "    limit_val_batches=1,\n",
    "    # limit_test_batches=10,\n",
    "    # limit_pool_batches=10,\n",
    "    # log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "results = trainer.active_fit(\n",
    "    model=entropy_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_df = pd.DataFrame(\n",
    "    data=[(l.data_stats[\"train_size\"], *l.test_outputs[0].values()) for l in results],\n",
    "    columns=(\"train_size\", *results[0].test_outputs[0].keys()),\n",
    ")\n",
    "entropy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import UserList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel()\n",
    "\n",
    "trainer = Trainer(\n",
    "    query_size=2,\n",
    "    max_epochs=3,\n",
    "    max_labelling_epochs=4,\n",
    "    total_budget=5,\n",
    "    log_every_n_steps=1,\n",
    "    test_after_labelling=True,\n",
    "    # for testing purposes\n",
    "    limit_train_batches=10,\n",
    "    limit_val_batches=10,\n",
    "    limit_test_batches=10,\n",
    "    limit_pool_batches=10,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel()\n",
    "\n",
    "trainer = Trainer(\n",
    "    query_size=2,\n",
    "    max_epochs=3,\n",
    "    max_labelling_epochs=4,\n",
    "    total_budget=5,\n",
    "    log_every_n_steps=1,\n",
    "    test_after_labelling=True,\n",
    "    # for testing purposes\n",
    "    limit_train_batches=10,\n",
    "    limit_val_batches=10,\n",
    "    limit_test_batches=10,\n",
    "    limit_pool_batches=10,\n",
    ")\n",
    "\n",
    "trainer.test(\n",
    "    model=model,\n",
    "    dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel()\n",
    "\n",
    "pl_trainer = PLTrainer(\n",
    "    max_epochs=3,\n",
    "    log_every_n_steps=1,\n",
    "    # for testing purposes\n",
    "    limit_train_batches=10,\n",
    "    limit_val_batches=10,\n",
    "    limit_test_batches=10,\n",
    ")\n",
    "\n",
    "results = pl_trainer.test(\n",
    "    model=model,\n",
    "    dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"c\".center(3, \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(\n",
    "    model=random_strategy.model,\n",
    "    dataloaders=test_dl,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('energizer-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf3d60d746ce6794d4ec556d1628bdd1ecace636e1760c52a7757d10f6e042be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
