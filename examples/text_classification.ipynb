{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0001\n",
    "MAX_LABELLING_ITERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer as PLTrainer\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy, F1Score, MetricCollection, Precision, Recall\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_constant_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from energizer import AccumulatorStrategy, RandomStrategy, Trainer\n",
    "from energizer.acquisition_functions import entropy, expected_entropy\n",
    "from energizer.data.datamodule import ActiveDataModuleWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ag_news (/home/pl487/.cache/huggingface/datasets/pietrolesci___ag_news/concat/1.0.0/5ee6e111adc7a901ca734b79fbebff09d9dba91722387a794efff8d9c178a6a3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b085a8996c5244869c4502e229d50d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/pl487/.cache/huggingface/datasets/pietrolesci___ag_news/concat/1.0.0/5ee6e111adc7a901ca734b79fbebff09d9dba91722387a794efff8d9c178a6a3/cache-244c58eb4fe55230.arrow\n",
      "Loading cached processed dataset at /home/pl487/.cache/huggingface/datasets/pietrolesci___ag_news/concat/1.0.0/5ee6e111adc7a901ca734b79fbebff09d9dba91722387a794efff8d9c178a6a3/cache-561b1c44fc31a694.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# renames \"label\" to \"labels\"\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(\"pietrolesci/ag_news\", \"concat\")\n",
    "\n",
    "# tokenize\n",
    "dataset = dataset.map(lambda ex: tokenizer(ex[\"text\"]), batched=True)\n",
    "columns_to_keep = [\"label\", \"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "\n",
    "# train-val split and record datasets\n",
    "train_set, test_set = dataset[\"train\"], dataset[\"test\"]\n",
    "_split = train_set.train_test_split(0.3)\n",
    "_, val_set = _split[\"train\"], _split[\"test\"]\n",
    "\n",
    "labels = train_set.features[\"label\"].names\n",
    "num_classes = len(labels)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = BATCH_SIZE\n",
    "eval_batch_size = EVAL_BATCH_SIZE  # this is use when evaluating on the pool too\n",
    "train_dl = DataLoader(\n",
    "    train_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = ActiveDataModuleWithIndex(\n",
    "    train_dataloader=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    "    faiss_index_path=\"train_ag_news.faiss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_classes: int,\n",
    "        learning_rate: float = 0.00001,\n",
    "        num_warmup_steps: int = 50,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_classes,\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        for stage in (\"train\", \"val\", \"test\"):\n",
    "            metrics = MetricCollection(\n",
    "                {\n",
    "                    \"accuracy\": Accuracy(),\n",
    "                    \"precision_macro\": Precision(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"precision_micro\": Precision(num_classes=num_classes, average=\"micro\"),\n",
    "                    \"recall_macro\": Recall(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"recall_micro\": Recall(num_classes=num_classes, average=\"micro\"),\n",
    "                    \"f1_macro\": F1Score(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"f1_micro\": F1Score(num_classes=num_classes, average=\"micro\"),\n",
    "                }\n",
    "            )\n",
    "            setattr(self, f\"{stage}_metrics\", metrics)\n",
    "\n",
    "    def common_step(self, batch: Any, stage: str):\n",
    "        \"\"\"Outputs loss and logits, logs loss and metrics.\"\"\"\n",
    "        out = self(batch)\n",
    "        logits, loss = out.logits, out.loss\n",
    "        self.log(f\"{stage}/loss\", loss)\n",
    "\n",
    "        metrics = getattr(self, f\"{stage}_metrics\")(logits, batch[\"labels\"])\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, batch) -> torch.Tensor:\n",
    "        return self.model(**batch)\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int = 0, optimizer_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        optimizer = AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.learning_rate)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": get_constant_schedule_with_warmup(\n",
    "                    optimizer=optimizer, num_warmup_steps=self.num_warmup_steps\n",
    "                ),\n",
    "                \"monitor\": \"val/loss\",\n",
    "                \"frequency\": 1,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(model_name=MODEL_NAME, num_classes=num_classes, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_strategy = RandomStrategy(deepcopy(model))\n",
    "\n",
    "seed_everything(1994)\n",
    "\n",
    "trainer = Trainer(\n",
    "    query_size=50,\n",
    "    max_epochs=3,\n",
    "    max_labelling_epochs=MAX_LABELLING_ITERS,\n",
    "    # total_budget=5,\n",
    "    test_after_labelling=True,\n",
    "    accelerator=\"gpu\",\n",
    "    # for testing purposes\n",
    "    # limit_train_batches=10,\n",
    "    limit_val_batches=1,\n",
    "    # limit_test_batches=10,\n",
    "    # limit_pool_batches=10,\n",
    "    # log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "results = trainer.active_fit(\n",
    "    model=random_strategy,\n",
    "    # train_dataloaders=train_dl,\n",
    "    # val_dataloaders=val_dl,\n",
    "    # test_dataloaders=test_dl,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = results.to_pandas()\n",
    "random_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AccumulatorStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyStrategy(AccumulatorStrategy):\n",
    "    \"\"\"A implememntation of the `Entropy` active learning strategy.\"\"\"\n",
    "\n",
    "    def get_inputs_from_batch(self, batch: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
    "        batch.pop(\"labels\")\n",
    "        return batch\n",
    "\n",
    "    def pool_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Tensor:\n",
    "        logits = self(batch).logits\n",
    "        return entropy(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_strategy = EntropyStrategy(deepcopy(model))\n",
    "\n",
    "seed_everything(1994)\n",
    "\n",
    "trainer = Trainer(\n",
    "    query_size=50,\n",
    "    max_epochs=3,\n",
    "    max_labelling_epochs=MAX_LABELLING_ITERS,\n",
    "    # total_budget=5,\n",
    "    test_after_labelling=True,\n",
    "    accelerator=\"gpu\",\n",
    "    # for testing purposes\n",
    "    # limit_train_batches=10,\n",
    "    limit_val_batches=1,\n",
    "    limit_test_batches=10,\n",
    "    limit_pool_batches=10,\n",
    "    # log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "results = trainer.active_fit(\n",
    "    model=entropy_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_df = pd.DataFrame(\n",
    "    data=[(l.data_stats[\"train_size\"], *l.test_outputs[0].values()) for l in results],\n",
    "    columns=(\"train_size\", *results[0].test_outputs[0].keys()),\n",
    ")\n",
    "entropy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnchorPointsStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energizer.query_strategies.base import RandomArchorPointsStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRandomArchorPointsStrategy(RandomArchorPointsStrategy):\n",
    "    def get_search_query_from_batch(self, batch: Any) -> Tensor:\n",
    "        return batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1994\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[1;36m[2022-08-27 21:25:07] energizer/DETAIL\u001b[0m ~ \u001b[1;33mtrainer:227\u001b[0m$ Trainer: trainer active_fit stage\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name          | Type                          | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model         | BertForSequenceClassification | 4.4 M \n",
      "1 | train_metrics | MetricCollection              | 0     \n",
      "2 | val_metrics   | MetricCollection              | 0     \n",
      "3 | test_metrics  | MetricCollection              | 0     \n",
      "----------------------------------------------------------------\n",
      "4.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.546    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e063b67c248648188207aed2cb05aeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[1;32m[2022-08-27 21:25:11] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `MyRandomArchorPointsStrategy`\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 0--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 21:25:11] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0542ead4f04c2c9d19bb19f08f3abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16093750298023224    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.12252724170684814    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16093750298023224    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_macro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.18885883688926697    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_micro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16093750298023224    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.15549920499324799    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_micro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16093750298023224    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4066975116729736     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16093750298023224   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12252724170684814   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16093750298023224   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_macro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.18885883688926697   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_micro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16093750298023224   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.15549920499324799   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_micro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16093750298023224   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4066975116729736    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 21:25:11] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `MyRandomArchorPointsStrategy`\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, pool_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[1;32m[2022-08-27 21:25:11] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:195\u001b[0m$ \u001b[37mQueried 50 instance\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:11] energizer/INFO\u001b[0m ~ \u001b[1;33mdatamodule:304\u001b[0m$ \u001b[37mUpdating `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:12] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:283\u001b[0m$ \u001b[37mAnnotated 50 instances\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:12] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:284\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 469\n",
      "num_train_batches: 2\n",
      "pool_size: 119950\n",
      "total_data_size: 120000\n",
      "train_size: 50\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Labelling Iteration 1--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 21:25:12] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:12] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:252\u001b[0m$ \u001b[37mTransformerModel state dict has been re-initialized\u001b[0m\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535ac9dc148a4e50ad2741a532db2fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2bef02bf8c4a62a6db602fdaafd4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2978143564f47fb97e500dca5f59527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12835f422d284400a200b0dca170b5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;32m[2022-08-27 21:25:14] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae30880c0e84f0ba8cbb2b762065557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16328124701976776    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.12923763692378998    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16328124701976776    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_macro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20363256335258484    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_micro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16328124701976776    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1578933298587799     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_micro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16328124701976776    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.405013084411621     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16328124701976776   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12923763692378998   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16328124701976776   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_macro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20363256335258484   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_micro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16328124701976776   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1578933298587799    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_micro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16328124701976776   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.405013084411621    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 21:25:15] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `MyRandomArchorPointsStrategy`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:15] energizer/INFO\u001b[0m ~ \u001b[1;33mdatamodule:298\u001b[0m$ \u001b[37mSearching `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:19] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:195\u001b[0m$ \u001b[37mQueried 500 instance\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:19] energizer/INFO\u001b[0m ~ \u001b[1;33mdatamodule:304\u001b[0m$ \u001b[37mUpdating `faiss_index`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:19] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:283\u001b[0m$ \u001b[37mAnnotated 500 instances\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:19] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:284\u001b[0m$ \u001b[37mNew data statistics\n",
      "num_pool_batches: 467\n",
      "num_train_batches: 18\n",
      "pool_size: 119450\n",
      "total_data_size: 120000\n",
      "train_size: 550\n",
      "\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:19] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n",
      "\u001b[1;32m[2022-08-27 21:25:19] energizer/INFO\u001b[0m ~ \u001b[1;33mactive_learning_loop:252\u001b[0m$ \u001b[37mTransformerModel state dict has been re-initialized\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Last fit_loop------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pl487/.conda/envs/energizer-dev/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58f56c1c7a649d7b7b4771391476356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 2it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d366e8e5b6fd43ea9e6062554d2b9d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31a45c4345a406f9270f6759c8eb674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb305a5057fd4811b517cb8bcf4342ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1;32m[2022-08-27 21:25:24] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:452\u001b[0m$ \u001b[37mUsing underlying `TransformerModel`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7d735717164426a58a85562cf3b2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.596484363079071     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5432099103927612     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.596484363079071     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_macro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6640535593032837     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      precision_micro      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.596484363079071     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5897459983825684     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       recall_micro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.596484363079071     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.184808373451233     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.596484363079071    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5432099103927612    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.596484363079071    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_macro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6640535593032837    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     precision_micro     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.596484363079071    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5897459983825684    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      recall_micro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.596484363079071    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.184808373451233    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2022-08-27 21:25:24] energizer/INFO\u001b[0m ~ \u001b[1;33mtrainer:448\u001b[0m$ \u001b[37mUsing `MyRandomArchorPointsStrategy`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "random_anchor_points_strategy = MyRandomArchorPointsStrategy(deepcopy(model), 10)\n",
    "datamodule = ActiveDataModuleWithIndex(\n",
    "    train_dataloader=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    "    faiss_index_path=\"train_ag_news.faiss\",\n",
    ")\n",
    "seed_everything(1994)\n",
    "\n",
    "trainer = Trainer(\n",
    "    query_size=50,\n",
    "    max_epochs=3,\n",
    "    max_labelling_epochs=MAX_LABELLING_ITERS,\n",
    "    # total_budget=5,\n",
    "    test_after_labelling=True,\n",
    "    accelerator=\"gpu\",\n",
    "    # for testing purposes\n",
    "    # limit_train_batches=10,\n",
    "    limit_val_batches=1,\n",
    "    limit_test_batches=10,\n",
    "    limit_pool_batches=10,\n",
    "    # log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "results = trainer.active_fit(\n",
    "    model=random_anchor_points_strategy,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>test/loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>recall_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.406698</td>\n",
       "      <td>0.160938</td>\n",
       "      <td>0.122527</td>\n",
       "      <td>0.160938</td>\n",
       "      <td>0.188859</td>\n",
       "      <td>0.160938</td>\n",
       "      <td>0.155499</td>\n",
       "      <td>0.160938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>1.405013</td>\n",
       "      <td>0.163281</td>\n",
       "      <td>0.129238</td>\n",
       "      <td>0.163281</td>\n",
       "      <td>0.203633</td>\n",
       "      <td>0.163281</td>\n",
       "      <td>0.157893</td>\n",
       "      <td>0.163281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550</td>\n",
       "      <td>1.184808</td>\n",
       "      <td>0.596484</td>\n",
       "      <td>0.543210</td>\n",
       "      <td>0.596484</td>\n",
       "      <td>0.664054</td>\n",
       "      <td>0.596484</td>\n",
       "      <td>0.589746</td>\n",
       "      <td>0.596484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_size  test/loss  accuracy  f1_macro  f1_micro  precision_macro  \\\n",
       "0           0   1.406698  0.160938  0.122527  0.160938         0.188859   \n",
       "1          50   1.405013  0.163281  0.129238  0.163281         0.203633   \n",
       "2         550   1.184808  0.596484  0.543210  0.596484         0.664054   \n",
       "\n",
       "   precision_micro  recall_macro  recall_micro  \n",
       "0         0.160938      0.155499      0.160938  \n",
       "1         0.163281      0.157893      0.163281  \n",
       "2         0.596484      0.589746      0.596484  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('energizer-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf3d60d746ce6794d4ec556d1628bdd1ecace636e1760c52a7757d10f6e042be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
