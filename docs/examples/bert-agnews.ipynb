{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "trainer_kwargs = {\n",
    "    \"query_size\": 1,\n",
    "    \"max_epochs\": 3,\n",
    "    \"max_labelling_epochs\": 5,\n",
    "    \"test_after_labelling\": True,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"limit_val_batches\": 1,\n",
    "    # total_budget=5,\n",
    "    # for testing purposes\n",
    "    # limit_train_batches=10,\n",
    "    # limit_test_batches=10,\n",
    "    # limit_pool_batches=10,\n",
    "    # log_every_n_steps=1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer as PLTrainer\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy, F1Score, MetricCollection, Precision, Recall\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_constant_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from energizer import Trainer\n",
    "from energizer.acquisition_functions import entropy, expected_entropy\n",
    "from energizer.query_strategies import RandomStrategy\n",
    "from energizer.query_strategies.base import AccumulatorStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# renames \"label\" to \"labels\"\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(\"pietrolesci/ag_news\", \"concat\")\n",
    "\n",
    "# tokenize\n",
    "dataset = dataset.map(lambda ex: tokenizer(ex[\"text\"]), batched=True)\n",
    "columns_to_keep = [\"label\", \"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "\n",
    "# train-val split and record datasets\n",
    "train_set, test_set = dataset[\"train\"], dataset[\"test\"]\n",
    "_split = train_set.train_test_split(0.3)\n",
    "_, val_set = _split[\"train\"], _split[\"test\"]\n",
    "\n",
    "labels = train_set.features[\"label\"].names\n",
    "num_classes = len(labels)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = BATCH_SIZE\n",
    "eval_batch_size = EVAL_BATCH_SIZE  # this is use when evaluating on the pool too\n",
    "train_dl = DataLoader(\n",
    "    train_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_set.with_format(columns=columns_to_keep),\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_classes: int,\n",
    "        learning_rate: float = 0.00001,\n",
    "        num_warmup_steps: int = 50,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_classes,\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        for stage in (\"train\", \"val\", \"test\"):\n",
    "            metrics = MetricCollection(\n",
    "                {\n",
    "                    \"accuracy\": Accuracy(),\n",
    "                    \"precision_macro\": Precision(\n",
    "                        num_classes=num_classes, average=\"macro\"\n",
    "                    ),\n",
    "                    \"recall_macro\": Recall(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"f1_macro\": F1Score(num_classes=num_classes, average=\"macro\"),\n",
    "                    \"f1_micro\": F1Score(num_classes=num_classes, average=\"micro\"),\n",
    "                }\n",
    "            )\n",
    "            setattr(self, f\"{stage}_metrics\", metrics)\n",
    "\n",
    "    def common_step(self, batch: Any, stage: str):\n",
    "        \"\"\"Outputs loss and logits, logs loss and metrics.\"\"\"\n",
    "        out = self(batch)\n",
    "        logits, loss = out.logits, out.loss\n",
    "        self.log(f\"{stage}/loss\", loss)\n",
    "\n",
    "        metrics = getattr(self, f\"{stage}_metrics\")(logits, batch[\"labels\"])\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, batch) -> torch.Tensor:\n",
    "        return self.model(**batch)\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Any, batch_idx: int = 0, optimizer_idx: int = 0\n",
    "    ) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int = 0) -> Dict[str, Any]:\n",
    "        return self.common_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        optimizer = AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.learning_rate,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": get_constant_schedule_with_warmup(\n",
    "                    optimizer=optimizer, num_warmup_steps=self.num_warmup_steps\n",
    "                ),\n",
    "                \"monitor\": \"val/loss\",\n",
    "                \"frequency\": 1,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    model_name=MODEL_NAME, num_classes=num_classes, learning_rate=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_strategy = RandomStrategy(deepcopy(model))\n",
    "\n",
    "seed_everything(1994)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=random_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")\n",
    "random_df = results.to_pandas()\n",
    "random_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AccumulatorStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyStrategy(AccumulatorStrategy):\n",
    "    \"\"\"A implememntation of the `Entropy` active learning strategy.\"\"\"\n",
    "\n",
    "    def get_inputs_from_batch(self, batch: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
    "        batch.pop(\"labels\")\n",
    "        return batch\n",
    "\n",
    "    def pool_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Tensor:\n",
    "        logits = self(batch).logits\n",
    "        return entropy(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_strategy = EntropyStrategy(deepcopy(model))\n",
    "\n",
    "seed_everything(1994)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=entropy_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")\n",
    "entropy_df = results.to_pandas()\n",
    "entropy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnchorPointsStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyRandomArchorPointsStrategy(RandomArchorPointsStrategy):\n",
    "#     def get_search_query_from_batch(self, batch: Any) -> Tensor:\n",
    "#         return batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_anchor_points_strategy = RandomArchorPointsStrategy(deepcopy(model), 10)\n",
    "\n",
    "# datastore = ActiveDataModuleWithIndex(\n",
    "#     train_dataloader=train_dl,\n",
    "#     val_dataloaders=val_dl,\n",
    "#     test_dataloaders=test_dl,\n",
    "#     faiss_index_path=\"all-mpnet-base-v2_ag-news_train.faiss\",\n",
    "# )\n",
    "\n",
    "# seed_everything(1994)\n",
    "# trainer = Trainer(**trainer_kwargs)\n",
    "# results = trainer.active_fit(\n",
    "#     model=random_anchor_points_strategy,\n",
    "#     datastore=datastore,\n",
    "# )\n",
    "# rap_df = results.to_pandas()\n",
    "# rap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(random_df[\"train_size\"], random_df[\"accuracy\"], label=\"random\")\n",
    "plt.plot(entropy_df[\"train_size\"], entropy_df[\"accuracy\"], label=\"entropy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df[\"strategy\"] = \"random\"\n",
    "entropy_df[\"strategy\"] = \"entropy\"\n",
    "results = pd.concat([random_df, entropy_df], ignore_index=False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.to_parquet(\"results_al.parquet\", index=False)\n",
    "# with open(\"results_al_metadata.json\", \"w\") as fl:\n",
    "#     json.dump(trainer_kwargs, fl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('energizer-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "944ffd4f8dbdaa69d919c942d599fc997735694f55f2ea363be8b6b69a40ba5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
