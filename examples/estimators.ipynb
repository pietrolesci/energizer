{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energizer.datastores.classification import PandasDataStoreForSequenceClassification\n",
    "from energizer.estimator import Estimator\n",
    "from energizer.utilities import move_to_cpu\n",
    "from energizer.enums import InputKeys, OutputKeys, RunningStage\n",
    "from energizer import seed_everything\n",
    "from energizer.callbacks import GradNorm\n",
    "from energizer.active_learning.datastores.classification import ActivePandasDataStoreForSequenceClassification\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import Accuracy, F1Score, Precision, Recall\n",
    "from datasets import load_dataset\n",
    "from energizer.active_learning.strategies.random import RandomStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(\"pietrolesci/agnews\")\n",
    "dataset_dict[\"train\"] = dataset_dict[\"train\"].select(range(1000))\n",
    "\n",
    "model_name = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset_dict = dataset_dict.map(lambda ex: tokenizer(ex[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function _from_datasets.<locals>.<lambda> at 0x7f4808e7a940> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b51ffa1b7dc4a00ac44f1a2b05e871f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c538fe057584d20842600e34ae2b886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = PandasDataStoreForSequenceClassification.from_dataset_dict(\n",
    "    dataset_dict=dataset_dict,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    target_name=\"labels\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorForSequenceClassification(Estimator):\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        stage: RunningStage,\n",
    "        model,\n",
    "        batch: Dict,\n",
    "        batch_idx: int,\n",
    "        loss_fn,\n",
    "        metrics: MetricCollection,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        _ = batch.pop(InputKeys.ON_CPU, None)\n",
    "\n",
    "        out = model(**batch)\n",
    "        out_metrics = metrics(out.logits, batch[InputKeys.TARGET])\n",
    "\n",
    "        if stage == RunningStage.TRAIN:\n",
    "            logs = {OutputKeys.LOSS: out.loss, **out_metrics}\n",
    "            self.log_dict({f\"{stage}/{k}\": v for k, v in logs.items()}, step=self.tracker.global_batch)\n",
    "\n",
    "        return out.loss\n",
    "    \n",
    "    def epoch_end(self, stage: RunningStage, output: List[np.ndarray], metrics: MetricCollection) -> float:\n",
    "        aggregated_metrics = move_to_cpu(metrics.compute())  # NOTE: metrics are still on device\n",
    "        aggregated_loss = round(np.mean(output).item(), 6)\n",
    "        \n",
    "        logs = {OutputKeys.LOSS: aggregated_loss, **aggregated_metrics}\n",
    "        self.log_dict({f\"{stage}_end/{k}\": v for k, v in logs.items()}, step=self.tracker.safe_global_epoch)\n",
    "\n",
    "        return aggregated_loss\n",
    "\n",
    "    def configure_metrics(self, *_) -> MetricCollection:\n",
    "        num_classes = self.model.num_labels\n",
    "        task = \"multiclass\"\n",
    "        # NOTE: you are in charge of moving it to the correct device\n",
    "        return MetricCollection(\n",
    "            {\n",
    "                \"accuracy\": Accuracy(task, num_classes=num_classes),\n",
    "                \"f1_macro\": F1Score(task, num_classes=num_classes, average=\"macro\"),\n",
    "                \"precision_macro\": Precision(task, num_classes=num_classes, average=\"macro\"),\n",
    "                \"recall_macro\": Recall(task, num_classes=num_classes, average=\"macro\"),\n",
    "                \"f1_micro\": F1Score(task, num_classes=num_classes, average=\"micro\"),\n",
    "                \"precision_micro\": Precision(task, num_classes=num_classes, average=\"micro\"),\n",
    "                \"recall_micro\": Recall(task, num_classes=num_classes, average=\"micro\"),\n",
    "            }\n",
    "        ).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593687c53fcb4a7aa43e70b9c4b9267c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimisation steps:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33829a74ad664b8cb169fe2ab683ace4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bb2f8071b94ca6b2a44d2ddb13e210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bdb960b9bb46f8990720e3e3d22e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c734f213874dad970582dbad68b03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.889831"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.prepare_for_loading()\n",
    "\n",
    "estimator = EstimatorForSequenceClassification(\n",
    "    model, \n",
    "    accelerator=\"gpu\",\n",
    "    tf32_mode=\"high\",\n",
    "    # loggers=[TensorBoardLogger(\"./\")],\n",
    "    # callbacks=[GradNorm(2), PytorchTensorboardProfiler(\"./profiler_logs\")],\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "    train_loader=ds.train_loader(),\n",
    "    validation_loader=ds.test_loader(),\n",
    "    validation_freq=\"1:step\",\n",
    "    limit_train_batches=5,\n",
    "    limit_validation_batches=1,\n",
    "    max_epochs=2,\n",
    "    learning_rate=0.001,\n",
    "    optimizer=\"adamw\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    scheduler=\"cosine_schedule_with_warmup\",\n",
    "    scheduler_kwargs={\"num_warmup_steps\": .1},\n",
    ")\n",
    "\n",
    "estimator.test(loader=ds.test_loader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = ActivePandasDataStoreForSequenceClassification.from_dataset_dict(\n",
    "    dataset_dict=dataset_dict,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    target_name=\"labels\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStrategyForSequenceClassification(EstimatorForSequenceClassification, RandomStrategy):\n",
    "    ...\n",
    "\n",
    "strategy = RandomStrategyForSequenceClassification(\n",
    "    model,\n",
    "    accelerator=\"gpu\",\n",
    "    tf32_mode=\"high\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bd640497d44787bcdc525375cb1826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimisation steps:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9aea6c536e742529a22731515b98c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89159f4963449879341880e723d09e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c917d309434b4044b562abf8cdf6284e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c322070974654eceb83fbef115a46040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.663024"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.prepare_for_loading()\n",
    "strategy.fit(\n",
    "    train_loader=ds.train_loader(),\n",
    "    validation_loader=ds.test_loader(),\n",
    "    validation_freq=\"1:step\",\n",
    "    limit_train_batches=5,\n",
    "    limit_validation_batches=1,\n",
    "    max_epochs=2,\n",
    "    learning_rate=0.001,\n",
    "    optimizer=\"adamw\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    scheduler=\"cosine_schedule_with_warmup\",\n",
    "    scheduler_kwargs={\"num_warmup_steps\": .1},\n",
    ")\n",
    "\n",
    "strategy.test(loader=ds.test_loader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607ee019cb934504adb94bc5e4ac4a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed rounds:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fb41c531b340358c7d888b0ff58792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimisation steps:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01272d8542a5475895c34a310faf2b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b31caeb81c4daba58a8598de7f6af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 40:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df6990d165a450e99a05746cbf52ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "_setup_round() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ads\u001b[39m.\u001b[39mprepare_for_loading()\n\u001b[0;32m----> 3\u001b[0m strategy\u001b[39m.\u001b[39;49mactive_fit(\n\u001b[1;32m      4\u001b[0m     datastore\u001b[39m=\u001b[39;49mads,\n\u001b[1;32m      5\u001b[0m     validation_freq\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m1:step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     limit_train_batches\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     limit_validation_batches\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     max_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39madamw\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     gradient_accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     scheduler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcosine_schedule_with_warmup\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     scheduler_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mnum_warmup_steps\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m.1\u001b[39;49m},\n\u001b[1;32m     14\u001b[0m     query_size\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/base.py:91\u001b[0m, in \u001b[0;36mActiveEstimator.active_fit\u001b[0;34m(self, datastore, query_size, max_rounds, max_budget, validation_perc, validation_sampling, reinit_model, model_cache_dir, max_epochs, min_epochs, max_steps, min_steps, validation_freq, gradient_accumulation_steps, learning_rate, optimizer, optimizer_kwargs, scheduler, scheduler_kwargs, log_interval, enable_progress_bar, limit_train_batches, limit_validation_batches, limit_test_batches, limit_pool_batches)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39msetup_active(\n\u001b[1;32m     65\u001b[0m     log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[1;32m     66\u001b[0m     enable_progress_bar\u001b[39m=\u001b[39menable_progress_bar,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     validation_perc\u001b[39m=\u001b[39mvalidation_perc,\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     75\u001b[0m fit_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m     76\u001b[0m     max_epochs\u001b[39m=\u001b[39mmax_epochs,\n\u001b[1;32m     77\u001b[0m     min_epochs\u001b[39m=\u001b[39mmin_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     limit_validation_batches\u001b[39m=\u001b[39mlimit_validation_batches,\n\u001b[1;32m     89\u001b[0m )\n\u001b[0;32m---> 91\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_active_fit(\n\u001b[1;32m     92\u001b[0m     datastore\u001b[39m=\u001b[39;49mdatastore,\n\u001b[1;32m     93\u001b[0m     query_size\u001b[39m=\u001b[39;49mquery_size,\n\u001b[1;32m     94\u001b[0m     replay\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     95\u001b[0m     reinit_model\u001b[39m=\u001b[39;49mreinit_model,\n\u001b[1;32m     96\u001b[0m     model_cache_dir\u001b[39m=\u001b[39;49mmodel_cache_dir,\n\u001b[1;32m     97\u001b[0m     validation_sampling\u001b[39m=\u001b[39;49mvalidation_sampling,\n\u001b[1;32m     98\u001b[0m     validation_perc\u001b[39m=\u001b[39;49mvalidation_perc,\n\u001b[1;32m     99\u001b[0m     limit_test_batches\u001b[39m=\u001b[39;49mlimit_test_batches,\n\u001b[1;32m    100\u001b[0m     limit_pool_batches\u001b[39m=\u001b[39;49mlimit_pool_batches,\n\u001b[1;32m    101\u001b[0m     fit_loop_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(\n\u001b[1;32m    102\u001b[0m         max_epochs\u001b[39m=\u001b[39;49mmax_epochs,\n\u001b[1;32m    103\u001b[0m         min_epochs\u001b[39m=\u001b[39;49mmin_epochs,\n\u001b[1;32m    104\u001b[0m         max_steps\u001b[39m=\u001b[39;49mmax_steps,\n\u001b[1;32m    105\u001b[0m         min_steps\u001b[39m=\u001b[39;49mmin_steps,\n\u001b[1;32m    106\u001b[0m         validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    107\u001b[0m         gradient_accumulation_steps\u001b[39m=\u001b[39;49mgradient_accumulation_steps,\n\u001b[1;32m    108\u001b[0m         limit_train_batches\u001b[39m=\u001b[39;49mlimit_train_batches,\n\u001b[1;32m    109\u001b[0m         limit_validation_batches\u001b[39m=\u001b[39;49mlimit_validation_batches,\n\u001b[1;32m    110\u001b[0m     ),\n\u001b[1;32m    111\u001b[0m     fit_opt_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(\n\u001b[1;32m    112\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    113\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    114\u001b[0m         optimizer_kwargs\u001b[39m=\u001b[39;49moptimizer_kwargs,\n\u001b[1;32m    115\u001b[0m         scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m    116\u001b[0m         scheduler_kwargs\u001b[39m=\u001b[39;49mscheduler_kwargs,\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m     ),\n\u001b[1;32m    119\u001b[0m )\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/base.py:139\u001b[0m, in \u001b[0;36mActiveEstimator.run_active_fit\u001b[0;34m(self, datastore, reinit_model, model_cache_dir, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_state_dict(model_cache_dir)\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback(\u001b[39m\"\u001b[39m\u001b[39mon_round_start\u001b[39m\u001b[39m\"\u001b[39m, datastore\u001b[39m=\u001b[39mdatastore)\n\u001b[0;32m--> 139\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_round(datastore\u001b[39m=\u001b[39;49mdatastore, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback(\u001b[39m\"\u001b[39m\u001b[39mon_round_end\u001b[39m\u001b[39m\"\u001b[39m, datastore\u001b[39m=\u001b[39mdatastore, output\u001b[39m=\u001b[39mout)\n\u001b[1;32m    142\u001b[0m output\u001b[39m.\u001b[39mappend(out)\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/base.py:175\u001b[0m, in \u001b[0;36mActiveEstimator.run_round\u001b[0;34m(self, datastore, query_size, replay, validation_perc, validation_sampling, limit_test_batches, limit_pool_batches, fit_loop_kwargs, fit_opt_kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_round\u001b[39m(\n\u001b[1;32m    163\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    164\u001b[0m     datastore: ActiveDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     fit_opt_kwargs: Dict,\n\u001b[1;32m    173\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ROUND_OUTPUT:\n\u001b[0;32m--> 175\u001b[0m     model, train_loader, validation_loader, optimizer, scheduler, test_loader, pool_loader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_round(\n\u001b[1;32m    176\u001b[0m         datastore,\n\u001b[1;32m    177\u001b[0m         replay,\n\u001b[1;32m    178\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_opt_kwargs,\n\u001b[1;32m    179\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_loop_kwargs,\n\u001b[1;32m    180\u001b[0m         limit_test_batches\u001b[39m=\u001b[39;49mlimit_test_batches,\n\u001b[1;32m    181\u001b[0m         limit_pool_batches\u001b[39m=\u001b[39;49mlimit_pool_batches,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    184\u001b[0m     output \u001b[39m=\u001b[39m {}\n\u001b[1;32m    186\u001b[0m     \u001b[39m# fit\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: _setup_round() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "ads.prepare_for_loading()\n",
    "\n",
    "strategy.active_fit(\n",
    "    datastore=ads,\n",
    "    validation_freq=\"1:step\",\n",
    "    limit_train_batches=5,\n",
    "    limit_validation_batches=1,\n",
    "    max_epochs=2,\n",
    "    learning_rate=0.001,\n",
    "    optimizer=\"adamw\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    scheduler=\"cosine_schedule_with_warmup\",\n",
    "    scheduler_kwargs={\"num_warmup_steps\": .1},\n",
    "    query_size=15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
