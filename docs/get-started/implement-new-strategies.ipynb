{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy, F1Score, MetricCollection, Precision, Recall\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from energizer import Trainer\n",
    "from energizer.query_strategies import EntropyStrategy, LeastConfidenceStrategy, MarginStrategy, RandomStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess data, and prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess datasets\n",
    "data_dir = \"./data\"\n",
    "preprocessing_pipe = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "train_set = MNIST(data_dir, train=True, download=True, transform=preprocessing_pipe)\n",
    "test_set = MNIST(data_dir, train=False, download=True, transform=preprocessing_pipe)\n",
    "train_set, val_set = random_split(train_set, [55000, 5000])\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 32\n",
    "eval_batch_size = 128  # this is use when evaluating on the pool too\n",
    "train_dl = DataLoader(train_set, batch_size=batch_size, num_workers=os.cpu_count())\n",
    "val_dl = DataLoader(val_set, batch_size=eval_batch_size, num_workers=os.cpu_count())\n",
    "test_dl = DataLoader(test_set, batch_size=eval_batch_size, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "        for stage in (\"train\", \"val\", \"test\"):\n",
    "            setattr(self, f\"{stage}_accuracy\", Accuracy())\n",
    "\n",
    "    def forward(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        # NOTE: here I am unpacking the batch in the forward pass\n",
    "        x, _ = batch\n",
    "        return self.model(x)\n",
    "\n",
    "    def loss(self, logits: Tensor, targets: Tensor) -> Tensor:\n",
    "        return F.cross_entropy(logits, targets)\n",
    "\n",
    "    def common_step(self, batch: Tuple[Tensor, Tensor], stage: str) -> Dict[str, Tensor]:\n",
    "        logits = self(batch)\n",
    "        loss = self.loss(logits, y)\n",
    "        accuracy = getattr(self, f\"{stage}_accuracy\")(logits, y)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(f\"{stage}/accuracy\", accuracy, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        return self.common_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        return self.common_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        return self.common_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self) -> None:\n",
    "        return torch.optim.SGD(self.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define active learning strategies\n",
    "\n",
    "We implement the following strategies:\n",
    "\n",
    "- `RandomStrategy`: selects random instances from the pool. Therefore, it does not need to run any computation on the pool. Thus, we inherit from the `NoAccumulatorStrategy` base class so that we can speed up the computations. As it does not need to run on the pool so we do not need to implement the `pool_step` method, we only need to implement the `query` method.\n",
    "\n",
    "- `EntropyStrategy`: selects instances that the model is most uncertain about, where the uncertainty is defined as the entropy of the predicted probability distribution over the classes. It needs to run operations on the pool, thus we inherit from the `AccumulatorStrategy` base class. Since it needs to run on the pool, we need to implement the `pool_step` method, but we do not need to implement `query` as `AccumulatorStrategy` knows how to compute the top-k operation and return the indices.\n",
    "\n",
    "- `LeastConfidenceStrategy`: selects instances that the model is most uncertain about, where the uncertainty is defined as the value of the smallest class probability. It needs to run operations on the pool like the `EntropyStrategy` \n",
    "\n",
    "- `MarginConfidenceStrategy`: selects instances that the model is most uncertain about, where the uncertainty is defined as the difference between the first and the second biggest highest class probabilities. It needs to run operations on the pool like the `EntropyStrategy` \n",
    "\n",
    "\n",
    "Note that both strategies are already available in the library directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStrategy(NoAccumulatorStrategy):\n",
    "    def query(self) -> List[int]:\n",
    "        pool_size = self.trainer.datamodule.pool_size\n",
    "        return np.random.randint(low=0, high=pool_size, size=self.query_size).tolist()\n",
    "\n",
    "\n",
    "class EntropyStrategy(AccumulatorStrategy):\n",
    "    def pool_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Tensor:\n",
    "        \"\"\"NOTE: since we are defining the `pool_step` ourselves, we can define\n",
    "        the logic to unpack the batch directly here. When using a pre-defined strategy,\n",
    "        we need to implement the `get_inputs_from_batch` hook, unless the forward\n",
    "        method of the model you defined is able to run on the batch \"as-is\" from the\n",
    "        dataloader.\n",
    "        \"\"\"\n",
    "        x, _ = batch\n",
    "        logits = self(x)\n",
    "        # use the entropy scoring function\n",
    "        scores = entropy(logits)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class LeastConfidenceStrategy(AccumulatorStrategy):\n",
    "    def pool_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Tensor:\n",
    "        x, _ = batch\n",
    "        logits = self(x)\n",
    "        # use the entropy scoring function\n",
    "        scores = least_confidence(logits)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class MarginStrategy(AccumulatorStrategy):\n",
    "    def pool_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Tensor:\n",
    "        x, _ = batch\n",
    "        logits = self(x)\n",
    "        # use the entropy scoring function\n",
    "        scores = margin_confidence(logits)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class ExpectedMarginStrategy(MCAccumulatorStrategy):\n",
    "    def pool_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Tensor:\n",
    "        x, _ = batch\n",
    "        logits = self(x)\n",
    "        # use the entropy scoring function\n",
    "        scores = expected_margin_confidence(logits)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class ExpectedEntropyStrategy(MCAccumulatorStrategy):\n",
    "    def pool_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Tensor:\n",
    "        x, _ = batch\n",
    "        logits = self(x)\n",
    "        # use the entropy scoring function\n",
    "        scores = expected_entropy(logits)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class BALDStrategy(MCAccumulatorStrategy):\n",
    "    def pool_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Tensor:\n",
    "        x, _ = batch\n",
    "        logits = self(x)\n",
    "        # use the entropy scoring function\n",
    "        scores = bald(logits)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(torch.tensor([0.01, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(torch.tensor([0.5, 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: when passing a model to build a strategy, internally a `deepcopy` will be created. This is done to avoid modifying the model state and passing it around when trying other strategies. It avoids messing up benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_strategy = RandomStrategy(model)\n",
    "entropy_strategy = EntropyStrategy(model)\n",
    "leastconfidence_strategy = LeastConfidenceStrategy(model)\n",
    "margin_strategy = MarginStrategy(model)\n",
    "expected_entropy_strategy = ExpectedEntropyStrategy(model)\n",
    "expected_margin_strategy = ExpectedMarginStrategy(model)\n",
    "bald_strategy = BALDStrategy(model)\n",
    "\n",
    "strategies = [\n",
    "    random_strategy,\n",
    "    entropy_strategy,\n",
    "    leastconfidence_strategy,\n",
    "    margin_strategy,\n",
    "    expected_entropy_strategy,\n",
    "    expected_margin_strategy,\n",
    "    bald_strategy,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass of the strategy internally calls the forward of the underlying module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dl))[0]\n",
    "\n",
    "model.eval()\n",
    "for strategy in strategies:\n",
    "    strategy.eval()\n",
    "\n",
    "out_model = model(x)\n",
    "out_random = random_strategy(x)\n",
    "out_entropy = entropy_strategy(x)\n",
    "\n",
    "model.train()\n",
    "random_strategy.train()\n",
    "entropy_strategy.train()\n",
    "\n",
    "assert torch.all(out_model == out_random)\n",
    "assert torch.all(out_model == out_entropy)\n",
    "\n",
    "out_model.shape, out_random.shape, out_entropy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity let's pack the trainer kwargs in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = {\n",
    "    \"query_size\": 10,  # 50 new instances will be queried at each iteration\n",
    "    \"max_epochs\": 3,  # the underlying model will be fit for 3 epochs\n",
    "    \"max_labelling_epochs\": 50,  # how many times to run the active learning loop\n",
    "    \"accelerator\": \"gpu\",  # use the gpu\n",
    "    \"test_after_labelling\": True,  # since we have a test set, we test after each labelling iteration\n",
    "    \"limit_val_batches\": 0,  # do not validate\n",
    "    \"log_every_n_steps\": 1,  # we will have a few batches while training, so log on each\n",
    "}\n",
    "\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)  # for reproducibility (e.g., dropout)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=random_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = results.to_pandas()\n",
    "results_dict[\"random\"] = random_df\n",
    "random_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)  # for reproducibility (e.g., dropout)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=entropy_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_df = results.to_pandas()\n",
    "results_dict[\"entropy\"] = entropy_df\n",
    "entropy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least confidence strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)  # for reproducibility (e.g., dropout)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=leastconfidence_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leastconfidence_df = results.to_pandas()\n",
    "results_dict[\"leastconfidence\"] = leastconfidence_df\n",
    "leastconfidence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)  # for reproducibility (e.g., dropout)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=margin_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_df = results.to_pandas()\n",
    "results_dict[\"margin\"] = margin_df\n",
    "margin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected entropy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)  # for reproducibility (e.g., dropout)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=expected_entropy_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_entropy_df = results.to_pandas()\n",
    "results_dict[\"expected_entropy\"] = expected_entropy_df\n",
    "expected_entropy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected margin confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)  # for reproducibility (e.g., dropout)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=expected_margin_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_margin_df = results.to_pandas()\n",
    "results_dict[\"expected_margin\"] = expected_margin_df\n",
    "expected_margin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)  # for reproducibility (e.g., dropout)\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "results = trainer.active_fit(\n",
    "    model=bald_strategy,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    test_dataloaders=test_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bald_df = results.to_pandas()\n",
    "results_dict[\"bald\"] = bald_df\n",
    "bald_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Now let's look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results_dict.items():\n",
    "    plt.plot(v[\"train_size\"], v[\"test/accuracy_epoch\"], label=k)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results_dict.items():\n",
    "    plt.plot(v[\"train_size\"], v[\"test/loss_epoch\"], label=k)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('energizer-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf3d60d746ce6794d4ec556d1628bdd1ecace636e1760c52a7757d10f6e042be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
