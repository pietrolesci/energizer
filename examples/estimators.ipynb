{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energizer.datastores.classification import PandasDataStoreForSequenceClassification\n",
    "from energizer.estimator import Estimator\n",
    "from energizer.utilities import move_to_cpu\n",
    "from energizer.enums import InputKeys, OutputKeys, RunningStage\n",
    "from energizer import seed_everything\n",
    "from energizer.callbacks import GradNorm\n",
    "from energizer.active_learning.datastores.classification import ActivePandasDataStoreForSequenceClassification\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import Accuracy, F1Score, Precision, Recall\n",
    "from datasets import load_dataset\n",
    "from energizer.active_learning.strategies.random import RandomStrategy\n",
    "from energizer.active_learning.strategies.uncertainty import UncertaintyBasedStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(\"pietrolesci/agnews\")\n",
    "dataset_dict[\"train\"] = dataset_dict[\"train\"].select(range(1000))\n",
    "\n",
    "model_name = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset_dict = dataset_dict.map(lambda ex: tokenizer(ex[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function _from_datasets.<locals>.<lambda> at 0x7f0592ca9c10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8f2f1df1ee4d6aa26d12582c22c222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbf2651199b4567879cb18aaaf2b848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = PandasDataStoreForSequenceClassification.from_dataset_dict(\n",
    "    dataset_dict=dataset_dict,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    target_name=\"labels\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorForSequenceClassification(Estimator):\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        stage: RunningStage,\n",
    "        model,\n",
    "        batch: Dict,\n",
    "        batch_idx: int,\n",
    "        loss_fn,\n",
    "        metrics: MetricCollection,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        _ = batch.pop(InputKeys.ON_CPU, None)\n",
    "\n",
    "        out = model(**batch)\n",
    "        if stage == RunningStage.POOL:\n",
    "            return self.score_fn(out.logits)\n",
    "\n",
    "        out_metrics = metrics(out.logits, batch[InputKeys.TARGET])\n",
    "\n",
    "        if stage == RunningStage.TRAIN:\n",
    "            logs = {OutputKeys.LOSS: out.loss, **out_metrics}\n",
    "            self.log_dict({f\"{stage}/{k}\": v for k, v in logs.items()}, step=self.tracker.global_batch)\n",
    "\n",
    "        return out.loss\n",
    "    \n",
    "    def epoch_end(self, stage: RunningStage, output: List[np.ndarray], metrics: MetricCollection) -> float:\n",
    "        aggregated_metrics = move_to_cpu(metrics.compute())  # NOTE: metrics are still on device\n",
    "        aggregated_loss = round(np.mean(output).item(), 6)\n",
    "        \n",
    "        logs = {OutputKeys.LOSS: aggregated_loss, **aggregated_metrics}\n",
    "        self.log_dict({f\"{stage}_end/{k}\": v for k, v in logs.items()}, step=self.tracker.safe_global_epoch)\n",
    "\n",
    "        return aggregated_loss\n",
    "\n",
    "    def configure_metrics(self, *_) -> MetricCollection:\n",
    "        num_classes = self.model.num_labels\n",
    "        task = \"multiclass\"\n",
    "        # NOTE: you are in charge of moving it to the correct device\n",
    "        return MetricCollection(\n",
    "            {\n",
    "                \"accuracy\": Accuracy(task, num_classes=num_classes),\n",
    "                \"f1_macro\": F1Score(task, num_classes=num_classes, average=\"macro\"),\n",
    "                \"precision_macro\": Precision(task, num_classes=num_classes, average=\"macro\"),\n",
    "                \"recall_macro\": Recall(task, num_classes=num_classes, average=\"macro\"),\n",
    "                \"f1_micro\": F1Score(task, num_classes=num_classes, average=\"micro\"),\n",
    "                \"precision_micro\": Precision(task, num_classes=num_classes, average=\"micro\"),\n",
    "                \"recall_micro\": Recall(task, num_classes=num_classes, average=\"micro\"),\n",
    "            }\n",
    "        ).to(self.device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f424da8997410c9407a73112627f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimisation steps:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0371375a3eb74efc83ec9043e3503d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ae9504e0b04c73b79991f610279fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757baa096d7c447381d94917cc9bf6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be98d4369e2f4a1cb7220f103611c896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.370891"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.prepare_for_loading()\n",
    "\n",
    "estimator = EstimatorForSequenceClassification(\n",
    "    model, \n",
    "    accelerator=\"gpu\",\n",
    "    tf32_mode=\"high\",\n",
    "    # loggers=[TensorBoardLogger(\"./\")],\n",
    "    # callbacks=[GradNorm(2), PytorchTensorboardProfiler(\"./profiler_logs\")],\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "    train_loader=ds.train_loader(),\n",
    "    validation_loader=ds.test_loader(),\n",
    "    validation_freq=\"1:step\",\n",
    "    limit_train_batches=5,\n",
    "    limit_validation_batches=1,\n",
    "    max_epochs=2,\n",
    "    learning_rate=0.001,\n",
    "    optimizer=\"adamw\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    scheduler=\"cosine_schedule_with_warmup\",\n",
    "    scheduler_kwargs={\"num_warmup_steps\": .1},\n",
    ")\n",
    "\n",
    "estimator.test(loader=ds.test_loader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5f286a22b246688bd5424f2e25dd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed rounds:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a00db9ae7e47dd835aa5d1a9df9945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labelled:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ec3e7a678b403b833a7418a25f9078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimisation steps: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaf458f29ca4ac6bdd076e4bf86f2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed epochs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c45c6107b648129d8e6daa557c141d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22b5a32b5804408a13500e1863ced4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{<RunningStage.TEST: 'test'>: 1.253059},\n",
       " {<RunningStage.TRAIN: 'train'>: [(1.218048, []), (1.256889, [])],\n",
       "  <RunningStage.TEST: 'test'>: 1.253059},\n",
       " {<RunningStage.TRAIN: 'train'>: [(1.312426, []), (1.313902, [])],\n",
       "  <RunningStage.TEST: 'test'>: 1.253059},\n",
       " {<RunningStage.TRAIN: 'train'>: [(1.267999, []), (1.27012, [])],\n",
       "  <RunningStage.TEST: 'test'>: 1.115026},\n",
       " {<RunningStage.TRAIN: 'train'>: [(1.292171, []), (1.29874, [])],\n",
       "  <RunningStage.TEST: 'test'>: 1.17038},\n",
       " {<RunningStage.TRAIN: 'train'>: [(1.246048, []), (1.197541, [])],\n",
       "  <RunningStage.TEST: 'test'>: 1.086082}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomStrategyForSequenceClassification(EstimatorForSequenceClassification, RandomStrategy):\n",
    "    ...\n",
    "\n",
    "random = RandomStrategyForSequenceClassification(\n",
    "    model=model, accelerator=\"gpu\", tf32_mode=\"high\",\n",
    ")\n",
    "\n",
    "ads = ActivePandasDataStoreForSequenceClassification.from_dataset_dict(\n",
    "    dataset_dict=dataset_dict,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    target_name=\"labels\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "ads.prepare_for_loading()\n",
    "\n",
    "random.active_fit(\n",
    "    datastore=ads,\n",
    "    validation_freq=\"1:step\",\n",
    "    limit_train_batches=5,\n",
    "    limit_validation_batches=1,\n",
    "    max_epochs=2,\n",
    "    max_rounds=5,\n",
    "    learning_rate=0.001,\n",
    "    optimizer=\"adamw\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    scheduler=\"cosine_schedule_with_warmup\",\n",
    "    scheduler_kwargs={\"num_warmup_steps\": .1},\n",
    "    query_size=15,\n",
    "    limit_test_batches=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b975c71898746fb982c8bc3d915a137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed rounds:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7908fd64eee4053afea809db4cb9b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labelled:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51233e50d6ac4f9fbbf33b49832b81d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimisation steps: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72e23dbc3b5443ab31c21695a075be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed epochs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15283dc0ef14cc8ae3268ef650a94b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8976514efc30486ca51af40988964013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee46f158a9834bda8d4bfc0e6b540f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pool: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "pool_step() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m      9\u001b[0m ads \u001b[39m=\u001b[39m ActivePandasDataStoreForSequenceClassification\u001b[39m.\u001b[39mfrom_dataset_dict(\n\u001b[1;32m     10\u001b[0m     dataset_dict\u001b[39m=\u001b[39mdataset_dict,\n\u001b[1;32m     11\u001b[0m     input_names\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m     target_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m ads\u001b[39m.\u001b[39mprepare_for_loading()\n\u001b[0;32m---> 17\u001b[0m least_conf\u001b[39m.\u001b[39;49mactive_fit(\n\u001b[1;32m     18\u001b[0m     datastore\u001b[39m=\u001b[39;49mads,\n\u001b[1;32m     19\u001b[0m     validation_freq\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m1:step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m     limit_train_batches\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m     limit_validation_batches\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m     max_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m     max_rounds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39madamw\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m     gradient_accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m     scheduler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcosine_schedule_with_warmup\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m     scheduler_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mnum_warmup_steps\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m.1\u001b[39;49m},\n\u001b[1;32m     29\u001b[0m     query_size\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     limit_test_batches\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/base.py:74\u001b[0m, in \u001b[0;36mActiveEstimator.active_fit\u001b[0;34m(self, datastore, query_size, max_rounds, max_budget, validation_perc, validation_sampling, reinit_model, model_cache_dir, max_epochs, min_epochs, max_steps, min_steps, validation_freq, gradient_accumulation_steps, learning_rate, optimizer, optimizer_kwargs, scheduler, scheduler_kwargs, log_interval, enable_progress_bar, limit_train_batches, limit_validation_batches, limit_test_batches, limit_pool_batches)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m# configure progress tracking\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39msetup_active(\n\u001b[1;32m     64\u001b[0m     log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[1;32m     65\u001b[0m     enable_progress_bar\u001b[39m=\u001b[39menable_progress_bar,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     validation_perc\u001b[39m=\u001b[39mvalidation_perc,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_active_fit(\n\u001b[1;32m     75\u001b[0m     datastore,\n\u001b[1;32m     76\u001b[0m     reinit_model,\n\u001b[1;32m     77\u001b[0m     model_cache_dir,\n\u001b[1;32m     78\u001b[0m     query_size\u001b[39m=\u001b[39;49mquery_size,\n\u001b[1;32m     79\u001b[0m     replay\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     80\u001b[0m     validation_sampling\u001b[39m=\u001b[39;49mvalidation_sampling,\n\u001b[1;32m     81\u001b[0m     validation_perc\u001b[39m=\u001b[39;49mvalidation_perc,\n\u001b[1;32m     82\u001b[0m     limit_test_batches\u001b[39m=\u001b[39;49mlimit_test_batches,\n\u001b[1;32m     83\u001b[0m     limit_pool_batches\u001b[39m=\u001b[39;49mlimit_pool_batches,\n\u001b[1;32m     84\u001b[0m     fit_loop_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(\n\u001b[1;32m     85\u001b[0m         max_epochs\u001b[39m=\u001b[39;49mmax_epochs,\n\u001b[1;32m     86\u001b[0m         min_epochs\u001b[39m=\u001b[39;49mmin_epochs,\n\u001b[1;32m     87\u001b[0m         max_steps\u001b[39m=\u001b[39;49mmax_steps,\n\u001b[1;32m     88\u001b[0m         min_steps\u001b[39m=\u001b[39;49mmin_steps,\n\u001b[1;32m     89\u001b[0m         validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m     90\u001b[0m         gradient_accumulation_steps\u001b[39m=\u001b[39;49mgradient_accumulation_steps,\n\u001b[1;32m     91\u001b[0m         limit_train_batches\u001b[39m=\u001b[39;49mlimit_train_batches,\n\u001b[1;32m     92\u001b[0m         limit_validation_batches\u001b[39m=\u001b[39;49mlimit_validation_batches,\n\u001b[1;32m     93\u001b[0m     ),\n\u001b[1;32m     94\u001b[0m     fit_opt_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(\n\u001b[1;32m     95\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     96\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     97\u001b[0m         optimizer_kwargs\u001b[39m=\u001b[39;49moptimizer_kwargs,\n\u001b[1;32m     98\u001b[0m         scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m     99\u001b[0m         scheduler_kwargs\u001b[39m=\u001b[39;49mscheduler_kwargs,\n\u001b[1;32m    100\u001b[0m     ),\n\u001b[1;32m    101\u001b[0m )\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/base.py:121\u001b[0m, in \u001b[0;36mActiveEstimator.run_active_fit\u001b[0;34m(self, datastore, reinit_model, model_cache_dir, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_state_dict(model_cache_dir)\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback(\u001b[39m\"\u001b[39m\u001b[39mon_round_start\u001b[39m\u001b[39m\"\u001b[39m, datastore\u001b[39m=\u001b[39mdatastore)\n\u001b[0;32m--> 121\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_round(datastore, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback(\u001b[39m\"\u001b[39m\u001b[39mon_round_end\u001b[39m\u001b[39m\"\u001b[39m, datastore\u001b[39m=\u001b[39mdatastore, output\u001b[39m=\u001b[39mout)\n\u001b[1;32m    124\u001b[0m output\u001b[39m.\u001b[39mappend(out)\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/base.py:182\u001b[0m, in \u001b[0;36mActiveEstimator.run_round\u001b[0;34m(self, datastore, query_size, replay, validation_perc, validation_sampling, limit_test_batches, limit_pool_batches, fit_loop_kwargs, fit_opt_kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m n_labelled \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    178\u001b[0m     \u001b[39mnot\u001b[39;00m replay  \u001b[39m# do not annotate in replay\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39mis_last_round  \u001b[39m# last round is used only to test\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(pool_loader \u001b[39mor\u001b[39;00m []) \u001b[39m>\u001b[39m query_size  \u001b[39m# enough instances\u001b[39;00m\n\u001b[1;32m    181\u001b[0m ):\n\u001b[0;32m--> 182\u001b[0m     n_labelled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_annotation(model, pool_loader, datastore, query_size, validation_perc, validation_sampling)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39melif\u001b[39;00m replay:\n\u001b[1;32m    184\u001b[0m     n_labelled \u001b[39m=\u001b[39m datastore\u001b[39m.\u001b[39mquery_size(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39mglobal_round)\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/base.py:205\u001b[0m, in \u001b[0;36mActiveEstimator.run_annotation\u001b[0;34m(self, model, loader, datastore, query_size, validation_perc, validation_sampling)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_annotation\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     model: _FabricModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    202\u001b[0m     \u001b[39m# query\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback(\u001b[39m\"\u001b[39m\u001b[39mon_query_start\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39mmodel, datastore\u001b[39m=\u001b[39mdatastore)\n\u001b[0;32m--> 205\u001b[0m     indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_query(model, loader, query_size, datastore)\n\u001b[1;32m    207\u001b[0m     \u001b[39m# prevent to query more than available budget\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     remaining_budget \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(query_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39mbudget_tracker\u001b[39m.\u001b[39mget_remaining_budget())\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/uncertainty.py:27\u001b[0m, in \u001b[0;36mUncertaintyBasedStrategy.run_query\u001b[0;34m(self, model, loader, query_size, datastore)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_query\u001b[39m(\n\u001b[1;32m     21\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     22\u001b[0m     model: _FabricModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     datastore: ActiveDataStore,\n\u001b[1;32m     26\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_most_uncertain(model, loader, query_size)\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/uncertainty.py:31\u001b[0m, in \u001b[0;36mUncertaintyBasedStrategy.compute_most_uncertain\u001b[0;34m(self, model, loader, query_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_most_uncertain\u001b[39m(\u001b[39mself\u001b[39m, model: _FabricModule, loader: _FabricDataLoader, query_size: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[1;32m     30\u001b[0m     \u001b[39m# calls the pool_step and pool_epoch_end that we override\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     out: List[Dict] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_evaluation(model, loader, RunningStage\u001b[39m.\u001b[39;49mPOOL)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     _out \u001b[39m=\u001b[39m ld_to_dl(out)\n\u001b[1;32m     33\u001b[0m     scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(_out[OutputKeys\u001b[39m.\u001b[39mSCORES])\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/estimator.py:413\u001b[0m, in \u001b[0;36mEstimator.run_evaluation\u001b[0;34m(self, model, loader, stage)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mon_\u001b[39m\u001b[39m{\u001b[39;00mstage\u001b[39m}\u001b[39;00m\u001b[39m_batch_start\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39mmodel, batch\u001b[39m=\u001b[39mbatch, batch_idx\u001b[39m=\u001b[39mbatch_idx)\n\u001b[1;32m    412\u001b[0m \u001b[39m# run model on batch\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m batch_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluation_step(model, batch, batch_idx, loss_fn, metrics, stage)\n\u001b[1;32m    415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mon_\u001b[39m\u001b[39m{\u001b[39;00mstage\u001b[39m}\u001b[39;00m\u001b[39m_batch_end\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39mmodel, output\u001b[39m=\u001b[39mbatch_out, batch\u001b[39m=\u001b[39mbatch, batch_idx\u001b[39m=\u001b[39mbatch_idx)\n\u001b[1;32m    417\u001b[0m \u001b[39m# record output\u001b[39;00m\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/active_learning/strategies/uncertainty.py:54\u001b[0m, in \u001b[0;36mUncertaintyBasedStrategy.evaluation_step\u001b[0;34m(self, model, batch, batch_idx, loss_fn, metrics, stage)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m# keep IDs here in case user messes up in the function definition\u001b[39;00m\n\u001b[1;32m     53\u001b[0m ids \u001b[39m=\u001b[39m batch[InputKeys\u001b[39m.\u001b[39mON_CPU][SpecialKeys\u001b[39m.\u001b[39mID]\n\u001b[0;32m---> 54\u001b[0m pool_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool_step(model, batch, batch_idx, loss_fn, metrics)\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool_out, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     57\u001b[0m     pool_out \u001b[39m=\u001b[39m {OutputKeys\u001b[39m.\u001b[39mSCORES: pool_out}\n",
      "\u001b[0;31mTypeError\u001b[0m: pool_step() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "class UncertaintyStrategy(EstimatorForSequenceClassification, UncertaintyBasedStrategy):\n",
    "    def pool_step(self, model, batch, batch_idx: int, metrics):\n",
    "        return super().step(RunningStage.POOL, model, batch, batch_idx, None, metrics)\n",
    "\n",
    "\n",
    "least_conf = UncertaintyStrategy(\n",
    "    model=model, accelerator=\"gpu\", tf32_mode=\"high\", score_fn=\"least_confidence\",\n",
    ")\n",
    "ads = ActivePandasDataStoreForSequenceClassification.from_dataset_dict(\n",
    "    dataset_dict=dataset_dict,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    target_name=\"labels\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "ads.prepare_for_loading()\n",
    "least_conf.active_fit(\n",
    "    datastore=ads,\n",
    "    validation_freq=\"1:step\",\n",
    "    limit_train_batches=5,\n",
    "    limit_validation_batches=1,\n",
    "    max_epochs=2,\n",
    "    max_rounds=5,\n",
    "    learning_rate=0.001,\n",
    "    optimizer=\"adamw\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    scheduler=\"cosine_schedule_with_warmup\",\n",
    "    scheduler_kwargs={\"num_warmup_steps\": .1},\n",
    "    query_size=15,\n",
    "    limit_test_batches=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_conf.tracker.global_budget, least_conf.tracker.budget_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_conf.tracker.budget_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_conf.tracker.step_tracker.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
